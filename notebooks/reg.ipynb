{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art neural networks used in deep learning typically come with millions of parameters. Unsurprisingly, it is therefore rarely an issue to push the training error to 0. In particular, without any regularization there is instant death through *overfitting*. In this notebook, we discuss possible avenues for combatting overfitting via regularization methods tailored for DL-applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleanest and most effective way to avoid overfitting is to get more training data. For instance, when you do a search on *white mug* you obtain the following results. What could be the problem when training on this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mug.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately though, getting more data also happens to be the most expensive option and can in many cases even be infeasible. What you can do is to artificially inflate the data that you have. This is known as **data augmentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, data augmentation consists of taking samples from your training set and applying small random geometric perturbations. This could be rotations, shifts, flipping, color changes, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows you how simple it is to perform data augmentation in ``keras``. First, we load the MNIST data set and convert it to a good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform the actual data augmentation using the ``ImageDataGenerator``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the result of the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3765945b70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(datagen.flow(X_train, y_train).next()[0][0,:,:,0], cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^1$ and $L^2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in classical statistics, we can also try to regularize the model by restricting the size of coefficients. If we use the $L^2$-norm, this corresponds to ridge regression, whereas the $L^1$-norm gives us lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10,), kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to classical statistics, the effectivity of weight regularization is limited in deep learning. One reason for this is that the role of weights in neural networks is completely different to the one in classical statistics. Neural networks are overparametrized on purpose -- we do not aim to reduce the parameters to a small number of interpretable coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique specifically devised for the setting deep learning. It was introduced in 2014 by [Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov](http://jmlr.org/papers/v15/srivastava14a.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large neural networks tend to match training data ridiculously well by creating highly elaborate interdepencies between different activation patterns. When seeing a new image, these highly elaborate interdependencies break down and the model is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dropout__ prevents the development of intricate dependencies by randomly resetting outputs of groups of neurons to 0 during training. During production all weights are used, but are rescaled by the dropout retention probability to account the difference to the training setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is motivated from sexual reproduction in biology leading to a random mixing of genes. This leads to a robustness of the functioning of genes. Make sure to watch https://www.youtube.com/watch?v=DleXA5ADG78 for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "Srivastava, Nitish, et al. [Dropout: a simple way to prevent neural networks from\n",
    "overfitting](http://jmlr.org/papers/v15/srivastava14a.html), JMLR 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scaling rule can be established rigorously for basic architectures, see [Chapter 7.12 of the deep learning book](http://www.deeplearningbook.org/contents/regularization.html). It is a cousin of the *bagging* idea which lies at the basis of random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras documentation presents an example on how to use Dropout layers https://keras.io/getting-started/sequential-model-guide/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization**, developed by [Sergey Ioffe, Christian Szegedy](https://arxiv.org/abs/1502.03167), is based on a simple, yet universally accepted paradigm: Standardize your data!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backpropagation all layers are trained at the same time. This means, that the input for higher layers is unstable for a long time, since it comes from lower hidden layers that are themselves subject to the training process. That is, we experience an *internal coveriate shift*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most immediate approach is to standardize the inputs before activations are computed. However, this has to be done in  a way that is compatible with backpropagation, as the following [example](https://arxiv.org/abs/1502.03167) shows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that a neuron receives an input from a previous layer, where a bias $b$ was added to the previous output $u$, i.e., $x = u + b$. Moreover, suppose we have training data $\\{u_1, \\ldots, u_N\\}$ resulting in $\\{x_1, \\ldots, x_N\\}$ after addition of bias. Naïve normalization would replace the input $u + b$ of a neuron by \n",
    "$$ u + b - \\frac1N \\sum_{i \\le N} (u_i + b) = u - \\frac1N \\sum_{i \\le N}{u_i}$$\n",
    "Then, after a gradient step, the bias would be updated as $b + \\Delta b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes a peculiar behavior: Since we subtract again the mean value with the new bias, the change of $b$ by $\\Delta b$ did not have any effect on the output of the layer and the bias $b$ would grow to $\\infty$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the backpropagation algorithm is to normalize the data within every mini-batch and provide additional scaling and bias parameter $\\gamma$ and $\\beta$ that are part of the backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/batchnorm.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Sergey Ioffe, Christian Szegedy, [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stress that the normalization is done *separately over each input* to a neuron. That means, if a neuron has input $\\{x^{(1)}, \\ldots x^{(d)}\\}$, we obtain parameters $\\gamma^{(1)}, \\ldots, \\gamma^{(d)}$ and $\\beta^{(1)}, \\ldots, \\beta^{(d)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform backpropagation, it is important that the procedure of batch normalization is differentiable. We verify this here for the derivatives with respect to the new parameters $\\gamma$ and $\\beta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\ell}{\\partial \\gamma} = \\sum_{i \\le m} \\frac{\\partial \\ell}{\\partial y_i} \\widehat{x_i}\\quad\\text{ and }\\quad\\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i \\le m} \\frac{\\partial \\ell}{\\partial y_i}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During production the sample-mean and sample-variance over the mini-batches are replaced by the mean and variance over the entire "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization offers a variety of benefits for training deep nets:\n",
    "\n",
    "1. The normalization stabilizes the training process, thereby making it possible to use higher learning rates. Hence, this leads to a speed up of the training process.\n",
    "2. Since means and variances are computed on each batch, the need for other regularization methods such as dropout is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add batch normalization in Keras. Note that there is [no clear consensus](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) whether the batch normalization should be applied before or after the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
