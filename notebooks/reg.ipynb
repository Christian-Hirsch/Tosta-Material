{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art neural networks used in deep learning typically come with millions of weights. Unsurprisingly, it is therefore rarely an issue to push the training error to 0. In particular, without any regularization there is instant death through overfitting. In this notebook, we discuss possible avenues for combatting overfitting via regularization methods tailored for DL-applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleanest and most effective way to avoid overfitting is to get more labelled data. For instance, when you do a search on *white mug* you obtain the following results. What could be the problem when training on this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mug.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately though, getting more labelled data also happens to be the most expensive option and can in many cases even be infeasible. What you can do is to artificially inflate the labelled data at your disposal. This is known as **data augmentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, data augmentation consists of taking samples from the training set and applying small random geometric perturbations. This could be rotations, shifts, flipping, color changes, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example reveals how surprisingly simple it is to perform data augmentation in ``keras``. First, we load the MNIST data set and convert it to a good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = [np.expand_dims(dat, -1) for dat in [X_train, X_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform the actual data augmentation using the ``ImageDataGenerator``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the result of the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f05e6861dd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "pyplot.imshow(datagen.flow(X_train, y_train,\n",
    "                          seed=42).next()[0][0,:,:,0], cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_1$ & $L_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in classical statistics, we can also try to regularize the model by restricting suitable weight norms. For $L_2$-norm, this corresponds to [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), whereas $L_1$-norm yields [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics%29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10,), kernel_regularizer=regularizers.l2(1e-2)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to classical statistics, the effectiviness of weight regularization is limited in deep learning. On a second thought, this is not surprising, since the role of weights in deep nets is in stark contrast to the one in classical statistics. Deep nets are overparametrized on purpose -- we do not aim to reduce the weights to a small number of interpretable coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique specifically devised for deep learning. It was introduced in 2014 by [Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever & Ruslan Salakhutdinov](http://jmlr.org/papers/v15/srivastava14a.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep nets tend to fit to the training data ridiculously well by creating highly elaborate interdepencies between different activation patterns. When seeing a new image, these highly elaborate interdependencies break down and the model is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dropout__ prevents the development of intricate dependencies by randomly resetting outputs of groups of neurons to 0 during training. During production all weights are used, but are rescaled by the dropout retention probability to account the difference to the training setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is motivated from sexual reproduction in biology. This principle leads to a random mixing of genes, which ensures robustness in the face of changing environments. Make sure to watch https://www.youtube.com/watch?v=DleXA5ADG78 for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "Srivastava, Nitish, et al. [Dropout: a simple way to prevent neural networks from\n",
    "overfitting](http://jmlr.org/papers/v15/srivastava14a.html), JMLR 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scaling rule can be established rigorously for basic architectures, see [Chapter 7.12 of the deep learning book](http://www.deeplearningbook.org/contents/regularization.html). It is a cousin of the *bagging* idea which lies at the basis of random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras documentation presents an example on how to use Dropout layers https://keras.io/getting-started/sequential-model-guide/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization**, is the most modern and powerful regularization method for deep nets. It was developed by [Sergey Ioffe & Christian Szegedy](https://arxiv.org/abs/1502.03167) in 2015 and is based on a simple, yet universally applicable paradigm: Standardize your inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backpropagation all layers are trained simultaneously. In particular, the input for higher layers is unstable for a long time, since it comes from lower hidden layers that are themselves subject to the training process. That is, we experience an **internal covariate shift**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most immediate approach is to standardize the inputs before activations are computed. However, this has to be done in  a way that is compatible with backpropagation, as the following [example](https://arxiv.org/abs/1502.03167) shows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that a neuron receives an input from a lower layer, where a bias $b$ was added to the corresponding output $u$, i.e., $x = u + b$. Moreover, suppose we have training data $\\{u_1, \\ldots, u_N\\}$ resulting in $\\{x_1, \\ldots, x_N\\}$ after adding the bias. Naïve normalization would replace the input $u + b$ of a neuron by \n",
    "$$ u + b - \\frac1N \\sum_{i \\le N} (u_i + b) = u - \\frac1N \\sum_{i \\le N}{u_i}$$\n",
    "Then, after a gradient step, the bias would be updated as $b + \\Delta b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes a peculiar behavior: Since we subtract again the mean value with the new bias, the change of $b$ by $\\Delta b$ did not have any effect on the output of the layer and the bias $b$ would escape to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the backpropagation algorithm is to normalize the data within every mini-batch and provide additional scaling and bias parameter $\\gamma$ and $\\beta$ that are part of the backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/batchnorm.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Sergey Ioffe & Christian Szegedy: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stress that the normalization is done *separately over each input* to a neuron. That means, for a neuron with input $\\{x^{(1)}, \\ldots x^{(d)}\\}$ batch normalization introduces parameters $\\gamma^{(1)}, \\ldots, \\gamma^{(d)}$ and $\\beta^{(1)}, \\ldots, \\beta^{(d)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform backpropagation, it is important that the procedure of batch normalization is differentiable. We verify this here for the derivatives with respect to the new parameters $\\gamma$ and $\\beta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\ell}{\\partial \\gamma} = \\sum_{i \\le m} \\frac{\\partial \\ell}{\\partial y_i} \\widehat{x_i}\\quad\\text{ and }\\quad\\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i \\le m} \\frac{\\partial \\ell}{\\partial y_i}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During production the sample-mean and sample-variance over the mini-batches are replaced by the mean and variance over the entire "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization offers two crucial benefits for training deep nets:\n",
    "\n",
    "1. The normalization stabilizes the training process, thereby allowing for higher learning rates. Hence, the training is sped up.\n",
    "2. Since means and variances are computed on each batch, the need for other regularization methods is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add batch normalization in Keras. Note that there is [no clear consensus](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) whether the batch normalization should be applied before or after the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In batch normalization, compute the partial derivatives $\\partial \\ell/\\partial \\hat{x}_i$, $\\partial \\ell/\\partial x_i$, $\\partial \\ell/\\partial \\mu_B$ and $\\partial \\ell/\\partial \\sigma_B^2$.\n",
    "2. Implement dropout manually for a single-layer MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
