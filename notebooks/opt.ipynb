{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous notebook](./sgd.ipynb), we encountered stochastic-gradient descent (SGD) as the standard method to fit neural networks to data. Despite its simplicity, SGD is still the foundation of many state-of-the-art optimiziers. In this notebook, we learn how speed and accuracy of SGD can be improved with simple tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on Sebastian Ruder's wonderful [blog post](http://ruder.io/optimizing-gradient-descent/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several iterations of SGD the weights gradually approach the optimum. However, at a certain point the gradient steps become too coarse to achieve further improvements. Once this happens, the learning rate should be reduced to arrive at a better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Adagrad**, developed by [John Duchi, Elad Hazan, Yoram Singer](http://jmlr.org/papers/v12/duchi11a.html), achieves this goal by reducing the learning rate with successive gradient steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, writing $g^{(i)} = \\nabla \\ell(w^{(i)})$ for the gradient of the loss function in the $i$th iteration, we let \n",
    "$$G^{(i)} = {\\sum_{k \\le i} (g^{(i)})^2}$$\n",
    "denote the sum of squared gradients in the first $i$ steps. Then, the Adagrad update step becomes\n",
    "$$w^{(i+1)} = w^{(i)} - \\frac\\alpha{\\sqrt{G^{(i)}}}g^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual implementation, Adagrad introduces a separate learning rate for each weight, so as to take into account situations where some converge faster than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, Adagrad often results in a highly aggressive reduction of the learning rate and therefore into slow learning. **RMSProp** developed by [Geoff Hinton](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) resolves this problem by replacing the sum of squared gradients with a moving average:\n",
    "$$G^{(i+1)} = .9 \\, G^{(i)} + .1\\, g^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we put\n",
    "$$w^{(i+1)} = w^{(i)} - \\frac\\alpha{\\sqrt{G^{(i)}}}g^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentum** is a [classical technique](https://www.sciencedirect.com/science/article/pii/0041555364901375?via%3Dihub) in optimization for dampening oscillations appearing in the course of gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/momentum.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to smoothen the gradients by replacing gradients with moving averages. Using a parameter $\\beta<1$, we introduce the momentum vector\n",
    "$$m^{(i+1)} = \\beta m^{(i)} + g^{(i)}.$$\n",
    "and define the update step via\n",
    "$$w^{(i+1)} = w^{(i)} - \\alpha m^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabriel Goh's [Distill article](https://distill.pub/2017/momentum/) provides beautiful interactive illustrations and a mathematical proof for momentum on the vanilla SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam** was developed by [Diederik P. Kingma and Jimmy Ba](https://arxiv.org/abs/1412.6980) and augments RMSProp with momentum. More precisely, we put\n",
    "$$m^{(i+1)} = \\beta_1 m^{(i)} + (1 - \\beta_1) g^{(i)},$$\n",
    "$$G^{(i+1)} = \\beta_2 G^{(i)} + (1 - \\beta_2) (g^{(i)})^2,$$\n",
    "\n",
    "and then define the update step\n",
    "$$w^{(i+1)} = w^{(i)} - \\frac\\alpha{\\sqrt{G^{(i)}}}m^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implementation also contains bias-corrections for $m^{(i)}$ and $G^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-Order Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical optimization gradient descend approaches are typically superseded by methods incorporating information on the Hessian, such as the Newton-Raphson algorithm. This has the advantage of speeding up convergence substantially. There are three problems, why this reasoning does not carry over to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Even moderately-sized networks have millions of weights. That means, the number of entries in the Hessian goes into the trillions. This means the death, unless highly sophisticated heuristics are used to reduce the number of weights. \n",
    "2. The Hessian is typically [ill-conditioned in deep learning](https://arxiv.org/abs/1706.04454). That is, the vast majority of eigenvalues are close to 0 and matrix inversion is numerically volatile. \n",
    "3. The number of training examples that are available nowadays is typically very large, in particular if techniques like data augmentation are used. Spending more computational time on refined optimization methods means that the algorithm can see less training data. Experience shows that the time gained by faster convergence is typically not worth the price paid by seeing less examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras makes it very convenient to specify the optimization algorithm to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, input_shape=(10,)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
