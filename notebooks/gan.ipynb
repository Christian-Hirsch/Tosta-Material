{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebooks, we have met [RBMs](./rbm.ipynb) and [VAEs](./autoEnc.ipynb) as tools for letting our AI generate data. However, both approaches come with constraints that we have to be aware of. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single-layer architecture of RBMs is not powerful enough to generate data with a high inherent complexity. For instance, by refining the training process, RBMs can be made to produce realistically looking digits, but there is no hope of arriving at an image of a cat or bird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE are far more expressive than RBMs and can generate promising candidates of images with high inherent complexities, such as faces. However, the images obtained in this manner tend to be blurry\n",
    "<img id=\"gab\" src=\"images/celebA.png\"  width=\"500\">\n",
    "https://github.com/yzwxx/vae-celebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeper reason can be deduced from analyzing the ELBO that the VAE is trained to optimize. Here, the reconstruction error is given as $-\\mathbb{E}_{x \\sim \\mathsf{data}}\\mathbb{E}_{z \\sim q(z|x)}[\\log p(x|z)]$, which results in a massive penalty if we encounter a data point with very low probability under $p(\\cdot|z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A radically new concept for a generative model are **Generative Adversarial Networks (GANs)** as introduced in a [landmark paper](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf%20(https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) introduced by Ian Goodfellow et. al. in 2014. The idea as simple as it is intriguing. The generative task is set up as an adversarial competition between two neural networks: While the goal of the **generator** is to produce realistically-looking images of cats, the job of the **discriminator** is tell actual images from generated ones. Then, the generator can take false decisions of the discriminator as cues on how to produce better images.\n",
    "\n",
    "<img id=\"gab\" src=\"images/ganGame.png\"  width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is adapted from a wonderful [AC-GAN implementation](https://github.com/keras-team/keras/blob/master/examples/mnist_acgan.py) in the Keras example section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator versus Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not only that the concept behind GANs can be explained in one sentence to a man in the street, also the mathematical concept is just as elegant as $E = mc^2$! It is based on the following minimax problem:\n",
    "\\begin{align} \\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\mathsf{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{\\mathsf{noise}}}[\\log( 1 - D(G(z)))],\n",
    "\\end{align}\n",
    "where $G$ and $D$ denote the generator and the discriminator, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fixed generator, the inner maximization means that the discriminator is trained to minimize the cross-entropy error for the classification problem of true vs. fake images. The outer minimization means that the generator tries to make this task as difficult as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristically speaking, training the generator is more difficult than training the discriminator. For instance, for fixed generator, the inner maximization problem can be solved explicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**\n",
    "\n",
    "Fix $G$. The solution of the optimization problem\n",
    "$$ \\max_D \\mathbb{E}_{x \\sim p_{\\mathsf{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{\\mathsf{noise}}}[\\log(1 - D(G(z)))]$$\n",
    "is given by \n",
    "$$D(x) = \\frac{p_\\mathsf{data}(x)}{p_\\mathsf{data}(x) + p_{G}(x)},$$\n",
    "where $p_{G}$ is the probability density of $G(Z)$ with $Z \\sim p_{\\mathsf{noise}}$. The optimal value of the maximization problem is given by\n",
    "$$\\mathsf{KL}(p_\\mathsf{data}||(p_{\\mathsf{data}} + p_G)/2) + \\mathsf{KL}(p_G||(p_{\\mathsf{data}} + p_G)/2),$$\n",
    "the **Jensen-Shannon divergence** between $p_{\\mathsf{data}}$ and $p_G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, both discriminator and generator are implemented as deep nets. Here, are examples that we use for a code-example below to generate MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "\n",
    "def disc_cnn():\n",
    "    return Sequential([\n",
    "        Conv2D(32, 3, padding='same', strides=2,\n",
    "                       input_shape=(28, 28, 1)),\n",
    "        LeakyReLU(.2),\n",
    "        Dropout(.3),\n",
    "\n",
    "        Conv2D(64, 3, padding='same', strides=1),\n",
    "        LeakyReLU(.2),\n",
    "        Dropout(.3),\n",
    "\n",
    "        Conv2D(128, 3, padding='same', strides=2),\n",
    "        LeakyReLU(.2),\n",
    "        Dropout(.3),\n",
    "\n",
    "        Conv2D(256, 3, padding='same', strides=1),\n",
    "        LeakyReLU(.2),\n",
    "        Dropout(.3),\n",
    "\n",
    "        Flatten()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Conv2DTranspose, Reshape\n",
    "\n",
    "def gen_cnn(latent_size=100):\n",
    "    return Sequential([\n",
    "        Dense(3 * 3 * 384, input_dim=latent_size, activation='relu'),\n",
    "        Reshape((3, 3, 384)),\n",
    "\n",
    "        Conv2DTranspose(192, 5, strides=1, padding='valid',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2DTranspose(96, 5, strides=2, padding='same',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv2DTranspose(1, 5, strides=2, padding='same',\n",
    "                            activation='tanh',\n",
    "                            kernel_initializer='glorot_normal')    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input \n",
    "\n",
    "\n",
    "def build_discriminator(num_classes=10):\n",
    "    image = Input(shape = (28, 28, 1))\n",
    "    features = disc_cnn()(image)\n",
    "\n",
    "    fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
    "    aux = Dense(num_classes, activation='softmax', name='auxiliary')(features)\n",
    "    return Model(image, [fake, aux])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator model:\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "# build the discriminator\n",
    "print('Discriminator model:')\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(\n",
    "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the generator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, multiply\n",
    "\n",
    "def build_generator(latent_size=100, num_classes=10):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., 28, 28, 1)\n",
    "\n",
    "    # this is the z space commonly referred to in GAN papers\n",
    "    latent = Input(shape=(latent_size, ))\n",
    "\n",
    "    # this will be our label\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    cls = Flatten()(Embedding(num_classes, latent_size,\n",
    "                              embeddings_initializer='glorot_normal')(image_class))\n",
    "\n",
    "    # hadamard product between z-space and a class conditional embedding\n",
    "    h = multiply([latent, cls])\n",
    "\n",
    "    fake_image = gen_cnn()(h)\n",
    "\n",
    "    return Model([latent, image_class], fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the generator\n",
    "generator = build_generator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and third the combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size=100\n",
    "\n",
    "# get a fake image\n",
    "latent = Input(shape=(latent_size, ))\n",
    "image_class = Input(shape=(1,), dtype='int32')\n",
    "fake_image = generator([latent, image_class])\n",
    "\n",
    "#put it into the discriminator\n",
    "discriminator.trainable = False\n",
    "fake, aux = discriminator(fake_image)\n",
    "combined = Model([latent, image_class], [fake, aux])\n",
    "\n",
    "combined.compile(\n",
    "    optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "    loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), _ = mnist.load_data()\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        num_batches = int(x_train.shape[0] / batch_size) \n",
    "        \n",
    "        for idx in range(num_batches):\n",
    "            #train discriminator\n",
    "            train_disc(idx, x_train, y_train, discriminator, generator)\n",
    "            \n",
    "            #train generator\n",
    "            train_gen(idx, x_train, y_train, discriminator, generator)\n",
    "            \n",
    "        gen_pic(generator, x_train, y_train)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(idx, x_train, y_train, discriminator, generator):\n",
    "    #generate mini-batch data\n",
    "    x, y, aux_y = gen_disc_data(idx, x_train, generator)\n",
    "    \n",
    "    # set weights so that discriminator does not try to infer \n",
    "    # class from fake images\n",
    "    disc_sample_weight = [np.ones(2 * batch_size),\n",
    "                      np.concatenate((np.ones(batch_size) * 2,\n",
    "                                      np.zeros(batch_size)))]\n",
    "    #train discriminator\n",
    "    discriminator.train_on_batch(x, [y, aux_y], sample_weight=disc_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_gen(idx, x_train, y_train, discriminator, generator, num_classes=10, batch_size=100):\n",
    "    # make new noise. we generate 2 * batch size here such that we have\n",
    "    # the generator optimize over an identical number of images as the\n",
    "    # discriminator\n",
    "    noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n",
    "    sampled_labels = np.random.randint(0, num_classes, 2 * batch_size)\n",
    "\n",
    "    # we want to train the generator to trick the discriminator\n",
    "    # For the generator, we want all the {fake, not-fake} labels to say\n",
    "    # not-fake\n",
    "    trick = np.ones(2 * batch_size) * .95\n",
    "    combined.train_on_batch(\n",
    "        [noise, sampled_labels.reshape((-1, 1))],\n",
    "        [trick, sampled_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_disc_data(idx, x_train, generator, num_classes=10, batch_size=100):\n",
    "    #sample class labels\n",
    "    sampled_labels = np.random.randint(0, num_classes, batch_size)\n",
    "    \n",
    "    #generate data conditioned on labels\n",
    "    x = gen_img_pair(idx, x_train, sampled_labels, generator)\n",
    "    y = np.array([.95] * batch_size + [0] * batch_size)\n",
    "    aux_y = gen_class_targets(idx, y_train)\n",
    "    \n",
    "    return x, y, aux_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_img_pair(idx, x_train, sampled_labels, generator, batch_size=100, latent_size=100, num_classes=10):\n",
    "        # generate a new batch of noise\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, latent_size))\n",
    "\n",
    "        # get a batch of real images\n",
    "        image_batch = x_train[idx * batch_size:(idx + 1) * batch_size]\n",
    "\n",
    "        # generate a batch of fake images, using the generated labels as a\n",
    "        # conditioner. We reshape the sampled labels to be\n",
    "        # (batch_size, 1) so that we can feed them into the embedding\n",
    "        # layer as a length one sequence\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "        return np.concatenate((image_batch, generated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_class_targets(idx, y_train, num_classes=10, batch_size=100):\n",
    "    sampled_labels = np.random.randint(0, num_classes, batch_size)\n",
    "    label_batch = y_train[idx * batch_size:(idx + 1) * batch_size]\n",
    "    return np.concatenate((label_batch, sampled_labels), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some digits to display\n",
    "def gen_pic(generator, x_train, y_train, num_rows=40, latent_size=100, num_classes=10):\n",
    "    noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n",
    "                    (num_classes, 1))\n",
    "\n",
    "    sampled_labels = np.array([\n",
    "        [i] * num_rows for i in range(num_classes)\n",
    "    ]).reshape(-1, 1)\n",
    "\n",
    "    # get a batch to display\n",
    "    generated_images = generator.predict(\n",
    "        [noise, sampled_labels], verbose=0)\n",
    "\n",
    "    # prepare real images sorted by class label\n",
    "    real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n",
    "                          epoch * num_rows * num_classes]\n",
    "    indices = np.argsort(real_labels, axis=0)\n",
    "    real_images = x_train[(epoch - 1) * num_rows * num_classes:\n",
    "                          epoch * num_rows * num_classes][indices]\n",
    "\n",
    "    # display generated images, white separator, real images\n",
    "    img = np.concatenate(\n",
    "        (generated_images,\n",
    "         np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n",
    "         real_images))\n",
    "\n",
    "    # arrange them into a grid\n",
    "    img = (np.concatenate([r.reshape(-1, 28)\n",
    "                           for r in np.split(img, 2 * num_classes + 1)\n",
    "                           ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
    "    Image.fromarray(img).save(  'plot{0:.2f}_generated.png'.format(np.random.rand()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove the theorem about the optimal discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
