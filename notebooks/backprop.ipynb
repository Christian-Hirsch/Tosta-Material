{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in other regression models, we have to deal with the question of how to fit the parameters of an MLP. In the lingo of neural networks, we speak of **training the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the [previous lecture](mlp.ipynb) that our goal is to minimize a loss function $\\ell$ over training data $\\{(x_i, y_i)\\}_{i \\le N}$:\n",
    "$$L(w) = \\frac1N \\sum_{i \\le N} \\ell(y_i, f_w(x_i)).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even for convex $\\ell$, the subtle dependency of $f$ on $w$ via a sequence of matrix multiplications and non-linearities means that we are facing a daunting non-convex optimization problem. In addition, the number of weights can easily scale into the millions. In this lecture, we discuss numerical methods to perform efficient optimization under these extreme conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most naÃ¯ve numerical procedure for minimizing the loss function $L$ is **gradient descent**, where we iteratively change the weights by taking steps in the direction of the negative gradient $-\\nabla_w L$ of the loss function:\n",
    "$$w^{i+1} = w^{i} - \\alpha \\nabla_w L(w^{i}),$$\n",
    "where $\\alpha > 0$ is the **learning rate**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradientDesc.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Google Machine Learning Course](./images/learnRate.html) provides an enlightening illustration of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the Newton-Raphson algorithm, the learning rate is not computed as a function of the current weights $w^{(i)}$, but rather tuned manually. A too high learning rate can cause instabilities and failure of convergence, a too small learning rate leads to slow convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Gradient Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MLP $f_w$ consists of many hidden layers, then computing the gradient by hand is painful. A key feature of `tensorflow` is the **automated gradient computation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(tensorflow)\n",
    "library(keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it avoids a substantial amount of pain when it comes to forming derivatives, setting up the automatic gradient computation correctly requires a little bit of machinery. We illustrate this by computing the derivative of the square function $x \\mapsto x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- k_placeholder()\n",
    "square <- k_square(x)\n",
    "grads <- k_gradients(square, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, this sets up a **computational graph**, so that the engine knows how ``square`` depends on $x$. In order to actually perform the computation for specific input, we need to start a **session**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>-10.3999996185303</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item -10.3999996185303\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. -10.3999996185303\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] -10.4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())\n",
    "sess$run(grads, feed_dict = dict(x=-5.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply this principle to the linear model from the [previous lecture](./tensorflow.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim <- 5\n",
    "input_size <- 2\n",
    "\n",
    "x <- k_placeholder(c(input_size, input_dim))\n",
    "\n",
    "lm <- keras_model_sequential() %>%\n",
    "    layer_dense(1, input_shape = input_dim)\n",
    "lm_ph <- lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As loss function, we use `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y <- k_placeholder(c(input_size))\n",
    "learn_rate <- k_variable(1)\n",
    "\n",
    "loss <- k_mean(k_square(y - lm(x)))\n",
    "\n",
    "#extract weights from model\n",
    "weights <- lm$trainable_weights[[1]]\n",
    "\n",
    "#gradient of loss\n",
    "grads <- k_gradients(loss, weights)[[1]]\n",
    "grad_step <- k_update_sub(weights, learn_rate * grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these dependencies set up, we are now ready to perform gradient descent for specific input. We first specify mock training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix mock input data and compute loss\n",
    "set.seed(12)\n",
    "x_inp = matrix(runif(input_dim * input_size), input_size)\n",
    "y_inp = runif(input_size)\n",
    "\n",
    "var_dict <- dict(x=x_inp, y=y_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start a session and output the current weights, gradient and the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>-0.51848340</td></tr>\n",
       "\t<tr><td> 0.71340919</td></tr>\n",
       "\t<tr><td> 0.02645874</td></tr>\n",
       "\t<tr><td> 0.60588360</td></tr>\n",
       "\t<tr><td> 0.92671943</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t -0.51848340\\\\\n",
       "\t  0.71340919\\\\\n",
       "\t  0.02645874\\\\\n",
       "\t  0.60588360\\\\\n",
       "\t  0.92671943\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| -0.51848340 | \n",
       "|  0.71340919 | \n",
       "|  0.02645874 | \n",
       "|  0.60588360 | \n",
       "|  0.92671943 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]       \n",
       "[1,] -0.51848340\n",
       "[2,]  0.71340919\n",
       "[3,]  0.02645874\n",
       "[4,]  0.60588360\n",
       "[5,]  0.92671943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<tbody>\n",
       "\t<tr><td>-0.3463624418</td></tr>\n",
       "\t<tr><td> 0.0397177637</td></tr>\n",
       "\t<tr><td> 0.0134828379</td></tr>\n",
       "\t<tr><td>-0.2509757280</td></tr>\n",
       "\t<tr><td> 0.0001818177</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{l}\n",
       "\t -0.3463624418\\\\\n",
       "\t  0.0397177637\\\\\n",
       "\t  0.0134828379\\\\\n",
       "\t -0.2509757280\\\\\n",
       "\t  0.0001818177\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| -0.3463624418 | \n",
       "|  0.0397177637 | \n",
       "|  0.0134828379 | \n",
       "| -0.2509757280 | \n",
       "|  0.0001818177 | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "              [,1]\n",
       "[1,] -0.3463624418\n",
       "[2,]  0.0397177637\n",
       "[3,]  0.0134828379\n",
       "[4,] -0.2509757280\n",
       "[5,]  0.0001818177\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.15413361787796"
      ],
      "text/latex": [
       "0.15413361787796"
      ],
      "text/markdown": [
       "0.15413361787796"
      ],
      "text/plain": [
       "[1] 0.1541336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set up session\n",
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())\n",
    "\n",
    "sess$run(weights, feed_dict = var_dict)\n",
    "sess$run(grads, feed_dict = var_dict)\n",
    "\n",
    "sess$run(loss, feed_dict = var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we recompute the loss after applying a gradient step. Indeed, we see that the value has decreased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.3830212</td></tr>\n",
       "\t<tr><td>0.9663973</td></tr>\n",
       "\t<tr><td>0.5996777</td></tr>\n",
       "\t<tr><td>0.4440495</td></tr>\n",
       "\t<tr><td>0.4135252</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.3830212\\\\\n",
       "\t 0.9663973\\\\\n",
       "\t 0.5996777\\\\\n",
       "\t 0.4440495\\\\\n",
       "\t 0.4135252\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.3830212 | \n",
       "| 0.9663973 | \n",
       "| 0.5996777 | \n",
       "| 0.4440495 | \n",
       "| 0.4135252 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]     \n",
       "[1,] 0.3830212\n",
       "[2,] 0.9663973\n",
       "[3,] 0.5996777\n",
       "[4,] 0.4440495\n",
       "[5,] 0.4135252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.220877915620804"
      ],
      "text/latex": [
       "0.220877915620804"
      ],
      "text/markdown": [
       "0.220877915620804"
      ],
      "text/plain": [
       "[1] 0.2208779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess$run(grad_step, feed_dict = var_dict)\n",
    "sess$run(loss, feed_dict = var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when working in a regime with plenty of training data, then computing gradients iteratively can be prohibitively time-consuming. The idea of **stochastic gradient descent (SGD)** is to select only a subset $J$  of all training indices. The associated sub-sample is called **mini-batch**. Instead of computing the loss based on all training data, only the data in the mini-batch is used. That is, \n",
    "$$L_{\\mathsf{SGD}}(w) = \\sum_{i \\in J} \\ell(y_i, f_w(x_i)).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even with unlimited computational resources, we would typically prefer SGD over full gradient descent, since the implicit randomization helps to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `tensorflow` does a great job computing gradients for us. But what actually happens, when calling `k_gradients`? [Recall](./mlp.ipynb) that an MLP can be written in the form\n",
    "$$f_W(x) = a_L(W_L \\cdot a_{L-1}(W_{L-1}\\cdots (W_2 \\cdot a_1(W_1 \\cdot x + b_1) + b_2)\\cdots) + b_{L})$$\n",
    "with weights $W_i \\in \\mathbb{R}^{\\ell_i \\times \\ell_{i-1}}$, biases $b_i \\in \\mathbb{R}^{\\ell_i}$ and activation functions $a_i:\\mathbb{R}^{\\ell_i} \\to \\mathbb{R}^{\\ell_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write $f_W$ more succinctly as a composition of functions \n",
    "$$f_W(x) = f^{(L)}_{W_L, b_L}(\\cdots (f^{(2)}_{W_2, b_2}(f^{(1)}_{W_1, b_1}(x)))),$$\n",
    "where $f^{(i)}_{W_i, b_i}(z) = a_i(W_i z + b_i)$. Since $f$ is a composition of functions, the gradients can now be computes by invoking the chain rule. This technique of automatic differentiation is called **backpropagation** and goes back to a [seminal paper](https://www.nature.com/articles/323533a0) by David E. Rumelhart, Geoffrey E. Hinton & Ronald J. Williams published in 1986."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Google Machine Learning Crashcourse](./images/backProp.html) provides a beautiful illustration for backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain the general idea in the case where $L = 2$ without biases: $b_2 = b_1 = 0$. Since $a_2$ is typically a loss function, we assume the last layer to be one-dimensional, i.e., $\\ell_2=1$. To simplify notation, we write $\\ell = \\ell_1$ and $p = \\ell_0$. Hence, $W_2 = (w_{2;1}, w_{2;2}, \\ldots, w_{2;\\ell})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing $\\big(f^{(1)}_{W_1}(x)\\big)_k$ for the $k$th component of $f^{(1)}_{W_1}(x)$, the chain rule gives that\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_{2;j}}f_W(x)&= \\frac{\\partial}{\\partial w_{2;j}}a_2\\Big(W_2 f^{(1)}_{W_1}(x)\\Big)\\\\\n",
    "&= a_2'\\Big(W_2 f^{(1)}_{W_1}(x)\\Big) \\frac{\\partial}{\\partial w_{2;j}}\\sum_{k \\le \\ell} w_{2;k} \n",
    "\\big(f^{(1)}_{W_1}(x)\\big)_k\\\\\n",
    "&= a_2'\\Big(W_2 f^{(1)}_{W_1}(x)\\Big)\\big(f^{(1)}_{W_1}(x)\\big)_j\n",
    "\\end{align*}$$\n",
    "Hence, \n",
    "$$\\nabla_{W_2} f_W(x) =  a_2'(W_2 f^{(1)}_{W_1}(x))f^{(1)}_{W_1}(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expressions $W_2 f^{(1)}_{W_1}(x)$ and $f^{(1)}_{W_1}(x)$ were already evaluated when computing $f_W(x) = a_2(W_2 f_{W_1}^{(1)}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the partial derivatives with respect to $W_1$ is similar with the exception that we now also have to apply the chain rule for $g_{W_1}^{(1)}$:\n",
    "$$\\frac{\\partial}{\\partial w_{1;i,j}}a_2(W_2 f^{(1)}_{W_1}(x)) = a_2'\\Big(W_2 f^{(1)}_{W_1}(x)\\Big) \\frac{\\partial}{\\partial w_{1;i,j}}\\Big(W_2 f^{(1)}_{W_1}(x)\\Big)= a_2'\\Big(W_2 f^{(1)}_{W_1}(x)\\Big) W_2 \\frac{\\partial}{\\partial w_{1;i,j}}f^{(1)}_{W_1}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we again have already computed $a_2'\\Big(W_2 f^{(1)}_{W_1}(x)\\Big)$ when computing gradients with respect to $W_2$. Hence, we can proceed recursively and it only remains to compute the gradients for $f^{(1)}_{W_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculations already provide a very strong hint that when computing $f(x)$ and its gradients $f(x)$ many intermediate should be stored and re-used at a later stage. More precisely, the training via SGD decomposes into a **forward pass** and a **backward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute $f_W(x)= f^{(L)}_{W_L}(\\cdots (f^{(2)}_{W_2}(f^{(1)}_{W_1}(x))))$ recursively:\n",
    "\n",
    "* compute $f^{(1)}_{W_1}(x)$ and save the value as $h^{(1)}$\n",
    "* compute $f^{(2)}_{W_2}(h^{(1)})$ and save the value as $h^{(2)}$\n",
    "* continue recursively until arriving at $f_W(x) = f^{(L)}_{W_L}(h^{(L-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **forward phase** where we start with the computation of the innermost expression and make our way forward until arriving at the outermost position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **backward pass**, the gradients $\\nabla_W f_W(x)$ are computed via the chain rule. We put  $g^{(i)}_{W_i}(z) = a_i'(W_i z)$. Here, we start by computing the outermost expressions and make our way to the innermost expression. \n",
    "* To compute $\\nabla_{W_L} f_W(x)$, we proceed as above to see that $\\nabla_{W_L} f_W(x) = g^{(L)}_{W_L}\\big(h^{(L-1)}\\big) \\cdot h^{(L-1)}$. \n",
    "* To compute $\\nabla_{W_{L-1}} f_W(x)$, we proceed as above to see that $\\nabla_{W_{L-1}} f_W(x) =  g^{(L)}_{W_L}\\big(h^{(L-1)}\\big) \\cdot W_L\\cdot \\nabla_{W_{L-1}} f^{(L-1)}_{W_{L-1}}\\big(h^{(L-2)}\\big)$.  We store the expression $ g^{(L)}_{W_L}\\big(h^{(L-1)}\\big)$.\n",
    "* To compute  $\\nabla_{W_{L-2}} f_W(x)$, we proceed as above to see that \n",
    "$$\\nabla_{W_{L-2}} f_W(x) = g^{(L)}_{W_L}(h^{(L-1)}) \\cdot W_L\\cdot g^{(L-1)}_{W_{L-1}}(h^{(L-2)}) \\cdot W_{L-1} \\cdot \\nabla f_{W_{L-2}}^{(L-2)}(h^{(L-3)}).$$\n",
    "We can reuse the expression $ g^{(L)}_{W_L}\\big(h^{(L-1)}\\big)$ f_Wrom the previous step and store additionally the expression $ g^{(L-1)}_{W_{L-1}}\\big(h^{(L-2)}\\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already see that the computations can get rather involved. All the details are presented in  [Chapter 6.5 of the monograph on Deep Learning](http://www.deeplearningbook.org/) by  Aaron Courville, Ian J. Goodfellow, and Yoshua Bengio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upshot of the backpropagation algorithm is that one training step in the backpropagation algorithm takes essentially one forward pass and one backward pass through the network. Hence, it takes roughly twice as long as a simple evaluation. However, as we will see in the practical examples, many passes over the data might be needed until the weights have converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing & Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have walked through SGD, we can understand better why it was not possible to build deep nets in the 1980s and 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule causes many multiplications as a gradient travels through the multiple layers. If the multiplications are not precisely fine-tuned, then the gradients become numerically unstable. To understand what happens when plugging in numerically unstable values into a sigmoid function, let's plot it once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME\nBAQFBQUGBgYHBwcICAgJCQkKCgoLCwsMDAwNDQ0ODg4PDw8QEBARERESEhITExMUFBQVFRUW\nFhYXFxcYGBgZGRkaGhobGxscHBwdHR0eHh4fHx8gICAhISEiIiIjIyMkJCQlJSUmJiYnJyco\nKCgpKSkqKiorKyssLCwtLS0uLi4vLy8wMDAxMTEyMjIzMzM0NDQ1NTU2NjY3Nzc4ODg5OTk6\nOjo7Ozs8PDw9PT0+Pj4/Pz9AQEBBQUFCQkJDQ0NERERFRUVGRkZHR0dISEhJSUlKSkpLS0tM\nTExNTU1OTk5PT09QUFBRUVFSUlJTU1NUVFRVVVVWVlZXV1dYWFhZWVlaWlpbW1tcXFxdXV1e\nXl5fX19gYGBhYWFiYmJjY2NkZGRlZWVmZmZnZ2doaGhpaWlqampra2tsbGxtbW1ubm5vb29w\ncHBxcXFycnJzc3N0dHR1dXV2dnZ3d3d4eHh5eXl6enp7e3t8fHx9fX1+fn5/f3+AgICBgYGC\ngoKDg4OEhISFhYWGhoaHh4eIiIiJiYmKioqLi4uMjIyNjY2Ojo6Pj4+QkJCRkZGSkpKTk5OU\nlJSVlZWWlpaXl5eYmJiZmZmampqbm5ucnJydnZ2enp6fn5+goKChoaGioqKjo6OkpKSlpaWm\npqanp6eoqKipqamqqqqrq6usrKytra2urq6vr6+wsLCxsbGysrKzs7O0tLS1tbW2tra3t7e4\nuLi5ubm6urq7u7u8vLy9vb2+vr6/v7/AwMDBwcHCwsLDw8PExMTFxcXGxsbHx8fIyMjJycnK\nysrLy8vMzMzNzc3Ozs7Pz8/Q0NDR0dHS0tLT09PU1NTV1dXW1tbX19fY2NjZ2dna2trb29vc\n3Nzd3d3e3t7f39/g4ODh4eHi4uLj4+Pk5OTl5eXm5ubn5+fo6Ojp6enq6urr6+vs7Ozt7e3u\n7u7v7+/w8PDx8fHy8vLz8/P09PT19fX29vb39/f4+Pj5+fn6+vr7+/v8/Pz9/f3+/v7////i\nsF19AAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3deXwU9fnA8ScBwikgioonWi88\n6oG39da2HomAohCVQ/DAIoJK68FRvFBUbKuggIhXq8Wr8lMUi1ZRUJQgIIcICHInIBAghCSb\nzG+TkGSTbDLHPkP4znw+fySzM988WeO+X7sbNjtiEVHCSV1fAaIgBCQihYBEpBCQiBQCEpFC\nQCJSCEhECgGJSCE9SDs22RXJt13ioZztfkzNj/gxdZv9D8lDeYWbfZiavdOHoZtyC7N9mLrZ\nn1tW4TbbNT5Aytlgl1Vgu8RD27f5MTVS5MfUrTv8mJpnbfRh6uY8H4ZuyLU2+zB1oz+3LGur\n7Rog2QQkIAFJISABCUgKAQlIQFIISEACkkJAAhKQFAISkICkEJCABCSFgAQkICkEJCABSSEg\nAQlICgEJSEBSCEhAApJCQAISkBQCEpCApBCQgAQkhYAEJCApBCQgAUkhIAEJSAoBCUhAUghI\nQAKSQkACEpAUAhKQgKQQkIAEJIWABCQgKQQkIAFJISABCUgKAQlI/kAqSN+6aysyvlf3UfkV\nn4HkOCCFHVLe3CdSyyCN7TEzo/fIis9AchyQwg7pnZ43lkHa0fkry5rVcUvZZyA5D0hhh2RZ\nS8ogLUrdHn2glza77HN0V1F2tM0b7bIKbJd4KGe7H1MjRX5M3Zbrx9R861cfpm7J82Hoxlxr\niw9Tf/XnlmVttV2TAKQZHYs/pk8t+xz9sKl9tDFOJxEFpUj5lntI0zsVf0yfUvY5+mH7X6J9\nvNMuq8h2iYcKCvyYWmT5MbUg4sfUQl+ua36hH1MjVp4PU/NUb1nZ65Yt+m7G1MnvLsm3W5qb\nAKRFqTuiEtMyyj6XLeA5kn08R9qjniOtW5IxbfLb4597cvDdd3S7Nu2Cs0464tCWzaS8Z3x9\njpRz7UzLmtdhU9lnIDkPSHUK6Ze50ya9+uzD997a5arzT2q7T0OpWkrLgw478ZQLLknr1K1n\nv/4PfeMfpKmTLeuF25cu6/dMxWcgOQ5IuxvSmnmfTxz9SP/uV53b7oCUWDMNWh3227Mvvbrb\nnwYOeXr0hLc//t+sBUuqfK2fv7UbNCD6cG5sz+6j8ys+A8lxQNo9kJZ9OfG5wbd2OvuoFhVy\nGh90wvmpN9056KkX35ry9fyVDqbyEiGFgGQgpF++fOPJ/tedd1STMjzNjjrzyh73DB/z7udz\nV7mfCiSFgGQQpLXffjjijitPbLWLT/NjL+py7/CXP5zlAU9sQFIISCZAypw9cfjNFxxSr/Q3\nBW1/12XgP96Z4eRRm6OApBCQ9mxI67+ZcN/Vx5X+3q3VqR3v//v7czJVBscEJIWAtMdCWvzW\n4GtPLPkNXMPjUvs/9/ESs15rBySFgJQYpHmv3PuHA0sexh3f6cFXvyu/DwKSQkAKBaQV7w+5\nqk3JA7kL+o6Zvq7yQSApBKTAQ5o/rtdxxb9Q2PuSga9+H28BkBQCUqAhZTybfkTxY7lTe4+e\nWeMiICkEpMBCWvJSt8OiiJpeeN+k1bUuBJJCQAokpHWT7m4ffTjX9LJhU9bZLgaSQkAKHqSf\nX+y8t0i9Uwe8v8bRVCApBKSAQZo9/MIUkdY3vPST46lAUghIQYI09+H2SSLt+n/k6nUKQFII\nSIGBtGD4mcmSfNajs9xOBZJCQAoGpFWjz68nSac/Ms/DVCApBKQgQPq4e3ORk4bO9jYVSAoB\nyXhIi4YdI9L6T195ngokhYBkOKQpnVOkweWvrU1gKpAUApLJkNaMPlXk8GELE5sKJIWAZC6k\nH+5uLckXv5Hwn+QBSSEgmQrpu5tSZK9bvlGYCiSFgGQmpGmd6skhw5erTAWSQkAyEdJnaUnS\n9ulEfsEQG5AUApJ5kKZeJHLSBL13KwGSQkAyDdLX0XujsyZqTgWSQkAyC9LCbvXl2PG6U4Gk\nEJBMgrRuYCM5fIz2W9ABSSEgmQMpc+Q+sv8IrV8xVAQkhYBkDKSPTpYmD+kzApJKQDIE0uJb\nkuX3P+5JZ+yzCUgKAUkZ0rpHW8gx7+xZp760CUgKAUkX0rSTZa/iR3VAApJCYYW09i8p0mFB\n8RaQgKRQSCF9cZK0nlC6CSQgKRRKSGsHp0ja4l0XgAQkhcII6X/tpM2/yi8BCUgKhQ9S1vCG\nkr604jKQgKRQ6CAtS5Pm42J3AAlICoUN0keHSvuMSnuABCSFwgUp89EGSbdUeUUQkICkUKgg\nLThH2rxXdSeQgKRQmCB9cqBctrjaXiABSaEQQRrbOKlfnD87AhKQFAoNpPX9pOmEeAeABCSF\nwgLp5z/IYdPiHgESkBQKCaRvjpKzF8U/BCQgKRQOSO/uJbfWdAJlIAFJoVBAejGlwT9qPAgk\nICkUBkjDk5u8WfNRIAFJoeBDyhoorSbXchxIQFIo8JDW3ySH1np6CSABSaGgQ1p5mbSr/WTK\nQAKSQgGHtPQ0Of/n2pcACUgKBRvS0lOkwxqbNUACkkKBhrT0VOlQ0z8flQckICkUZEiOHAEJ\nSBoFGFL0cV1He0dAApJGwYVU7Gi9g3VAApJCgYW05GTp6uikR0ACkkJBhbTsRIeOgAQkjQIK\nafW50sXhSfiABCSFgglpfar83sHvGUoCEpAUCiak3nLaL07XAglICgUS0gBpt8TxVCABSaEg\nQhohB81xPhVIQFIogJDGJrea4WIqkICkUPAgvZXS7L9upgIJSAoFDtL05ilvu5oKJCApFDRI\nPx0uz7ibCiQgKRQwSGvPk74upwIJSAoFDFJPudjJC1VjAxKQFAoWpOFy9DK3U4EEJIUCBent\n+nt/63oqkICkUJAgzWrVoNppxOwDEpAUChCkn4+VJz1MBRKQFAoOpKzL5BYvU4EEJIWCA2mQ\nnOv0LycqBSQgKRQYSJMb7PuDp6lAApJCQYG0qE3yW96mAglICgUEUuZF8hePU4EEJIUCAukv\n8ju3r2goC0hAUigYkP5Tb78FXqcCybadtllF9mvcV1Dgx9QiB/897suP+DG10JfrmldYw4GV\nB9T/1PPUiJXn+WtryZ9blpVvtyTXB0g7tthlRWyXeCjX/ht7qLDIj6k5O/2YWmBl+zB1W378\n/ZsulKHep+ZZ27x/cY1l+3PLsnJs1/gAiYd29gXhod2f5dIs71N5aAckhQIA6ZP6bRYnMBVI\nQFLIfEgrj0yq5Zzl9gEJSAqZD6m39EpoKpCApJDxkN5OarsioalAApJCpkNaenD9jxObCiQg\nKWQ6pE5yb4JTgQQkhQyHNEF+uzbBqUACkkJmQ/qhVcNpiU4FEpAUMhpS1qXyWMJTgQQkhYyG\n9LSc5/C0fLUEJCApZDKk+S2afp/4VCABSSGTIaUqPLADEpBUMhjSG3KK1z/miw1IQFLIXEjL\nD6r/mcZUIAFJIXMh9Za7VKYCCUgKGQtpSr1DHJ+4vNaABCSFTIW07gR5S2cqkICkkKmQHpQu\nSlOBBCSFDIU0s1GrH5WmAglICpkJKesCeV5rKpCApJCZkEbJRWpTgQQkhYyEtHz/FPdn5qsp\nIAFJISMh9ZX+elOBBCSFTIT0bUqbxN6moVJAApJCJkK6VO83DRuABCSVDIQ0UU5P4I1VqwUk\nIClkHqQ1RyZ/ojkVSEBSyDxIQ6Sb6lQgAUkh4yAtbN5C6zUNpQEJSAoZB+l6eVR3KpCApJBp\nkP6bfEyib2RXJSABSSHDIGWeKhOVpwIJSAoZBul5uVJ7KpCApJBZkNa0bTBTeyqQgKSQWZAe\nkd7qU4EEJIWMgrRx36YL1KcCCUgKGQXpPhmoPxVIQFLIJEg/Nt3nZ/2pQAKSQiZB6iGP+zAV\nSEBSyCBI39Rvu0Z/KpCApJFBkK6Sf+oPBRKQVDIH0pSkU3aqD90AJCCpZA6k38l/4pyMOfGA\nBCSFjIH0ulwc76zmiQckIClkCqTMdsn/AxKQFAo3pDFybZyzmmsEJCApZAik9UfX+xpIQNIo\n1JCel+vjnNVcJSABSSEzIK0/KnqHBCQgaRRmSM+XnA0JSEBSKMSQondI32wAEpBUCjGk0aWn\n5wMSkBQKL6T1R5bcIQEJSBqFF9Io6VryGUhAUii0kMrukIAEJI1CC+k5SS/dABKQFAorpPVH\nNphVugUkICkUVkjPyg27toAEJIVCCmn94WV3SEACkkYhhTS29N+QigMSkBQKKaTfJn1Vtgkk\nICkUTkhvylXl20ACkkLhhHSOTC7fBhKQFAolpClyXsUFIAFJoVBCuiL2vGJAApJCYYQ0I/n4\nrIpLQAKSQmGE1EXGxVwCEpAUCiGkuSmHrYu5CCQgKRRCSLfJU7EXgQQkhcIH6aemrVfFXgYS\nkBQKH6Q/y+BKl4EEJIVCB2nlPnstrbQDSEBSKHSQHpW7Ku8AEpAUChuk9Yc0nF95D5CApFDY\nIL0kN1XZAyQgKRQ2SGfIF1X2AAlICoUM0lS5sOouIAFJoZBBukbeqLoLSEBSKFyQfkg5IrPq\nPiABSaFwQeovI6rtAxKQFAoVpFWtWv5SbSeQgKRQqCA9Jf2q7wSSH5Ai43t1H5Vfsjk9taS/\nWW8Vf+oAJBftoZDa1f+++k4g+QFpbI+ZGb1Hlmxuzoj2dZcZ1t+GRTdmA8lFeyakidIxzl4g\n+QBpR+evLGtWxy3lO0aPtayBkyqtAZJ9eyaki+WjOHuB5AOkRanbLasgrfzu5/tbog/z0h/q\n0XXY6uKLRdnRNm+0yyqwXeKhnO1+TI0U+TF1W64fU/OtXxP58pnJJ8fbvSUvkaE1lWtt8WHq\nr/7csqyttmtcQprRsfhj+tRdFwv7Ru+gslMfnj/3gR450cub2kcb42gS7XHdJm/W9VUwtkj5\nljNI0zsVf0yfsuvi1H7FMzYWWdb2az6Pbm7rE21Svl1Wke0SD0UifkwtsvyYGin0Y2pi1zWz\n6YE58fYX+HJdC60CP8b6c8uybG9aO11CWpS6IyonLWPXxbsmlx+54+2yLZ4j2bcnPkcaVOUv\nY8viOZIPz5Fyrp1pWfM6bNrFqlPx47lv+261rNzO3wDJeXsgpPUHN/4p7gEg+fHr7xduX7qs\n3zPRB3XF90Xj7yuB033o9wuG9i1/lAgk+/ZASK/IjfEPAMmXf5Ad27P76HzLGjQgeuGO10v2\nrRh8fbeRmyvutey/KZD2PEgXydT4B4DES4QUCguk75JPq+EIkICkUFgg9ZHnajgCJCApFBJI\nq/ZutbqGQ0ACkkIhgfR3ubOmQ0ACkkIhgXRy0syaDgEJSAqFA9IUubTGY0ACkkLhgNRF/lnj\nMSABSaFQQFrS+OD1NR4EEpAUCgWkv8qgmg8CCUgKhQFS1m9SFtV8FEhAUigMkP4tnWs5CiQg\nKRQGSH+UybUcBRKQFAoBpDn1jqvtMJCApFAIIA2ofPLlqgEJSAoFH9K6A5qtqO04kICkUPAh\nvVrt1GKVAxKQFAo+pD/IlFqPAwlICgUe0vz67WpfACQgKRR4SPfL8NoXAAlICgUdUlbbhvHf\nPKg8IAFJoaBDekuutVkBJCApFHRIHeQ/NiuABCSFAg5pcUrbLJslQAKSQgGH9FAN71McE5CA\npFDAIR1bf77dEiABSaFgQ/pQrrJdAyQgKRRsSF3lDds1QAKSQoGGtLzpgTW/V0NZQAKSQoGG\n9KQMtF8EJCApFGhIJyXPtl8EJCApFGRIX8jFDlYBCUgKBRlSb3nJwSogAUmhAENa02rvNQ6W\nAQlICgUY0gTp7WQZkICkUIAh/aGmk11WDkhAUii4kBY1ONbROiABSaHgQnpIhjlaByQgKRRc\nSMfbv161JCABSaHAQvpMfu9sIZCApFBgId3q6B+RNgAJSCoFFdLafRz9I9IGIAFJpaBCell6\nOVwJJCApFFRIl8snDlcCCUgKBRTS4pRjnC4FEpAUCiikR2SI06VAApJCAYV0Qr25TpcCCUgK\nBRPSNLnE8VQgAUmhYEK6XcY5ngokICkUSEjr9mu+yvFUIAFJoUBCel16OJ8KJCApFEhIV8tk\n51OBBCSFggjp50a275wfE5CApFAQIf3dydvZlQckICkUREjnydcupgIJSAoFENIP9dq7mQok\nICkUQEhD7E6/XDkgAUmhAEI6rv5CN1OBBCSFggfpS7nU1VQgAUmh4EG6U15wNRVIQFIocJAy\nD2qywtVUIAFJocBBek+udzcVSEBSKHCQbpCJ7qYCCUgKBQ3S6hat17mbCiQgKRQ0SOOlj8up\nQAKSQkGDdLl86nIqkICkUMAgLUk5yu1UIAFJoYBBelIecDsVSEBSKGCQzkzKcDsVSEBSKFiQ\nMpLOdD0VSEBSKFiQHpQnXU8FEpAUChakdvUXuZ4KJCApFChI012+8LskIAFJoUBBGiCj3E8F\nEpAUChSkIxoucz8VSEBSKEiQPpFUD1OBBCSFggTpdpngYSqQgKRQgCBltmnm/B2/KwISkBQK\nEKT33f5JX2lAApJCAYLUXd70MhVIQFIoOJDW7dtqrZepQAKSQsGB9Kabc7nEBCQgKRQcSNfJ\n+56mAglICgUG0urmB6z3NBVIdQRpZ65dVqHtEg8V5PsxtajIj6n5BX5MLaztZ/+G9PM2NS/i\n7etqL2Ll+TB1py//twos25tWjg+Qdmyxy4rYLvFQrv039lBhkR9Tc3b6MbXAyq75YJp85m3q\ntnxvX1d7edY2H6Zm+3PLsnJs1/gAiYd29u3+h3bLXZ2lLzYe2vEcSaGgQBold3ucCiQgKRQU\nSJfIlx6nAglICgUE0uIG7bxOBRKQFAoIpJHu34arLCABSaGAQDpfZnqdCiQgKRQMSD/W/63n\nqUACkkLBgDRCBnmeCiQgKRQMSOfKt56nAglICgUC0oJ6J3ufCiQgKRQISMNlqPepQAKSQoGA\ndHbSbO9TgQQkhYIAaUG99glMBRKQFAoCpMfkoQSmAglICgUB0hmJPLIDEpA0CgCkucmnJzIV\nSEBSKACQHpZHEpkKJCApFABIpyV9n8hUIAFJIfMhzUk6K6GpQAKSQuZD+qsMT2gqkICkkPmQ\nTk3+IaGpQAKSQsZDykg6J7GpQAKSQsZDGixPJDYVSEBSyHhIJyXPT2wqkICkkOmQMuR3CU4F\nEpAUMh3SEHkywalAApJCpkM6JdFHdkACkkaGQ5qTdG6iU4EEJIUMhzRMHk90KpCApJDhkE5L\n8F9jNwAJSCqZDWlu0tkJTwUSkBQyG9LD8ljCU4EEJIXMhnR60pyEpwIJSAoZDWl+8hmJTwUS\nkBQyGtJjif1tbGlAApJCRkM6K7G/jS0NSEBSyGRIC+qdpjAVSEBSyGRIj8swhalAApJCJkM6\nV2YpTAUSkBQyGNLCeqdoTAUSkBQyGNIIGaIxFUhAUshgSOfLdxpTgQQkhcyFtLj+SSpTgQQk\nhcyF9LQ8qDIVSEBSyFxIF8rXKlOBBCSFjIX0U4PjdaYCCUgKGQvpH3K/zlQgAUkhYyFdJtN1\npgIJSAqZCml5wyOVpgIJSAqZCukFuUdpKpCApJCpkK6Sz5WmAglIChkKaWWTQ7WmAglIChkK\n6SW5U2sqkICkkKGQOsoUralAApJCZkJas9eBWVpTgQQkhcyE9LrcpjYVSEBSyExIXeQDtalA\nApJCRkJau3fr9WpTgQQkhYyENFF66k0FEpAUMhLSTfKO3lQgAUkhEyGt37fVOr2pQAKSQiZC\n+o/coDgVSEBSyERIveVNxalAApJCBkLKOrD5GsWpQAKSQgZCmiydNacCCUgKGQipj7yiORVI\nQFLIPEhZhzRZpTkVSEBSyDxI/5U01alAApJC5kHqL2NVpwIJSAqZB+molJ9VpwIJSAoZB2ma\n/EF3KpCApJBxkP4iz+pOBRKQFDIO0nH1F+tOBRKQFDINUoZcoDwVSEBSyDRIQ+RJ5alAApJC\npkE6NXm+8lQgAUkhwyDNSzpTeyqQgKSQYZAek0e0pwIJSAoZBukcydCeCiQgKWQWpMX1T1af\nCiQgKWQWpJEySH0qkICkkFmQLlY6AXNsQAKSQkZB2pzSTn8qkBKAdOc3NSiJjO/VfVR+6fZb\nqdE6VN4HJEf5BOkVGag/FUgJQKovRw5ZHA/S2B4zM3qPLN3+27CMjIzZlfcByVE+QbpavtCf\nCqQEIG144cJkOe2ZdVUd7ej8lWXN6ril5MLASdX3AclR/kD6tfHhPkwFUmLPkdb87Sypd9mE\n7Eo7F6Vut6yCtNklF9If6tF12OrYfUXZ0TZvtMsqsF3ioZztfkyNFPkxdVuuH1P/JXf5MHVL\nng9DN+ZaW3yY+qs/tyxrq+2amiFFm3WiSKPO02L2zOhYImhq8cfs1Ifnz32gR07Mvk3to42J\nM4l2R9fLzLq+CmEtUr5VDdLqUZfUl8Pvvb2lPFuxc3qn4o/pU0q+dmORZW2/5vOYfdv6RJuU\nb5dVZLvEQ5GIH1OLLD+mRgp9GLqt+cF5Powt8OO65hdaBX6M9eeWZdnetHbWAGnJiLOS5OgH\nMqKb2We3rti/KHVHVFBaRsWeO96uuo/nSPb58hzptegjOx/G8hwpgedIIicM/WHX9j2HVuzP\nuTb64GFeh03F29/23WpZuZ2/id0HJGf5AqmLfAEkH4YmAunRHyu2CyMxB164femyfs9Y1tTJ\nVk73od8vGNo3Ur4PSI7zA9LavVtHgOTDUF9e2RAZ27P76HzLGjTAslYMvr7byM0V+4DkOD8g\nTZTeFpB8GMpLhDQyB1J3+RBIQEq8kEPK3L/FNiABKfFCDmmSXJ8HJCAlXsgh3SqvAQlICoUb\nUtbBTVcBCUgKhRvSJ3L1BiABSaFwQ+on44AEJI3CDenIlJ+BBCSNQg1pmvxxA5CApFGoIQ0s\nPpkLkICkUKghtWvwE5CApFKYIc2UizYACUgqhRnSIHlqA5CApFKYIZ1ScjIXIAFJoRBDmpt0\ndvEnIAFJoRBDerT0ZC5AApJCIYZ0TtL3xZ+ABCSFwgtpYb32JZ+BBCSFwgvpSRla8hlIQFIo\nvJAukO9KPgMJSAqFFtLi+ieWbgAJSAqFFtI/5IHSDSABSaHQQrpMppduAAlICoUV0vKGx+za\nAhKQFAorpOfl3l1bQAKSQmGFdGX5afqABCSFQgppZZPDyjaBBCSFQgppvPQr2wQSkBQKKaQO\n8knZJpCApFA4Ia3Z68Cssm0gAUmhcEJ6TW4v3wYSkBQKJ6Tr5cPybSABSaFQQlq7936Z5ReA\nBCSFQgnp33JzxQUgAUmhUEK6Ud6tuAAkICkURkjr9221ruISkICkUBghvSc3xlwCEpAUCiOk\nXvJmzCUgAUmhEELK3L/FmpiLQAKSQiGENEm6xF4EEpAUCiGk3vLP2ItAApJC4YOU2Wav1bGX\ngQQkhcIH6QO5rtJlIAFJofBBuk1eq3QZSEBSKHSQsg5uVumRHZCApFHoIE2WayrvABKQFAod\npD7ycuUdQAKSQmGDlHVI45WV9wAJSAqFDdIU6VBlD5CApFDYIPWVl6rsARKQFAobpLaNf6my\nB0hAUihkkKZKatVdQAKSQiGDdJeMq7oLSEBSKGSQDm+4vOouIAFJoXBB+kyuqrYPSEBSKFyQ\nBsiYavuABCSFwgXpqJSfq+0DEpAUChWkafLH6juBBCSFQgXpbhlVfSeQgKRQqCAdlbKs+k4g\nAUmhMEH6TC6PsxdIQFIoTJD6ywtx9gIJSAqFCdLhDav/zg5IQFIpRJCmxvnX2A1AApJKIYJ0\nZ/XX2RUHJCApFCJIbRuviLcbSEBSKDyQpkha3P1AApJC4YF0h4yPux9IQFIoNJCyDqn2t7Gl\nAQlICoUG0mTpGP8AkICkUGgg3S4T4h8AEpAUCgukrIObrop/BEhAUigskD6o+k7F5QEJSAqF\nBdIt8moNR4AEJIVCAimzTZVzUFQEJCApFBJIk6qcXSwmIAFJoZBA6lX5vLGxAQlICoUDUub+\nLdbUdAxIQFIoHJD+I11qPAYkICkUDkjd5N81HgMSkBQKBaS1rVqtrfEgkICkUCggvSE9az4I\nJCApFApInWVSzQeBFChIO3Ptsgptl3ioIN+PqUVFfkzNL/D2dZubt8mp+Wihg5+9+/IiPgzN\njVh5Pkzd6cv/rQLL9qaV4wOkHVvssiK2SzyUa/+NPVRY5MfUnJ3evu4V6VvL0QIr29vYWtuW\n78PQLXnWNh+mZvtzy7JybNf4AImHdvZ5fWh3lUyt5SgP7QL10A5I9nmEtLxR29oOAwlICoUA\n0nNyT22HgQQkhUIA6WL5srbDQAKSQsGHtLjB8bUeBxKQFAo+pCdlUK3HgQQkhYIP6eykjFqP\nAwlICgUe0g/Jp9e+AEhAUijwkB6Sx2pfACQgKRR4SCcn/1D7AiABSaGgQ/ou6XybFUACkkJB\nh3S/PGOzAkhAUijokI5tsNhmBZCApFDAIX0hv7dbAiQgKRRwSHfKGLslQAKSQsGGlHlQk/gn\nRYoJSEBSKNiQ3q3lbbjKAhKQFAo2pHR5y3YNkICkUKAhrW6x/3rbRUACkkKBhvSi9LFfBCQg\nKRRoSH+UT+0XAQlICgUZ0k8pRztYBSQgKRRkSCPkQQergAQkhYIM6QybP+krDUhAUijAkGYn\nne1kGZCApFCAId0nTztZBiQgKRRgSEel2L3wuyQgAUmh4EL6RK50tA5IQFIouJBulQmO1gEJ\nSAoFFtL6/ZqvcrQQSEBSKLCQ/i03OVsIJCApFFhIneV9ZwuBBCSFggppRdODM52tBBKQFAoq\npGelv8OVQAKSQkGFdK7McLgSSEBSKKCQZtu943dFQAKSQgGF9GdnLw8qDkhAUiiYkLLaNlzq\ndC2QgKRQMCFNkk6OpwIJSAoFE1K6THQ8FUhAUiiQkH5pdoD9uweVBSQgKRRISM/JXc6nAglI\nCgUS0nky3flUIAFJoSBCmluvvYupQAKSQkGEdJ+McDEVSEBSKIiQjnT2N+a7AhKQFAogpA/k\najdTgQQkhQII6UZ5w81UIAFJoeBBWtWi9To3U4EEJIWCB+l56etqKpCApFDwIF0oX7qaCiQg\nKRQ4SLOTT3E3FUhAUihwkG7O9PYAABJsSURBVO5x/pdIpQEJSAoFDVLmwU1+djcVSEBSKGiQ\n/iXpLqcCCUgKBQ3SFTLZ5VQgAUmhgEFa2OAot1OBBCSFAgZpkDzidiqQgKRQsCBlHZHyo9up\nQAKSQsGC9J50dD0VSEBSKFiQrpF3XE8FEpAUChSkpY0OdfjO+TEBCUgKBQrSY/Kg+6lAApJC\ngYJ0fP0f3E8FEpAUChKkj+WPHqYCCUgKBQnSjfK6h6lAApJCAYK0otl+rv40dldAApJCAYL0\njAzwMhVIQFIoQJBOTZ7lZSqQgKRQcCBNlYs8TQUSkBQKDqR0ec3TVCABSaHAQFra+GDnp3KJ\nDUhAUigwkB7y8qqG4oAEJIWCAinrNykLvU0FEpAUCgqkiXKNx6lAApJCQYF0hXzgcSqQgKRQ\nQCDNrX9MlsepQAKSQgGBdI+rc4tVCkhAUigYkNYe0Mzl20JWBCQgKRQMSOOkl+epQAKSQsGA\ndI5M8zwVSEBSKBCQpied430qkICkUCAg9ZJx3qcCyRxIkfG9uo/KL93ePLJbl6HLLeut1Ggd\ngOSimiCt2Gu/Nd6nAskcSGN7zMzoPbJ0e1C/eYsfT99k/W1YRkbGbCC5qCZII+SeBKYCyRhI\nOzp/ZVmzOm4p3t6Yuih6D5X+sTVwUqU1QLKvBkhZ7erPSWAqkIyBtCh1u2UVpJXc/WT9K/oQ\nb+e1k630h3p0Hba6eFdRdrTNG+2yCmyXeChnux9TI0V+TN2WG3f3W9Ihkan51q+JfHkNbcnz\nYejGXGuLD1N/9eeWZW21XeMS0oyOxR/Tp5Zd3vl4z63ZqQ/Pn/tAj5zoxU3to41xNInidLnM\nqOurQJ6KlG85gzS9U/HH9Cmll4o+7XnfFiuysciytl/zeXTHtj7RJuXbZRXZLvFQJOLH1CLL\nj6mRwnh7FyS3T2iqP9e1IO51TbRCq8CPsf7csizbm9ZOl5AWpe6I6kvLKLmw5f6bPy8qO3LH\n22VbPEeyL/5zpJ4yJqGpPEcy5jlSzrUzLWteh00l90cDHi5+OGd923erZeV2/gZIzosLaUmT\nNmsTmgokYyBZL9y+dFm/Zyxr6mRrTtrnc6JtyOk+9PsFQ/uWP0oEkn1xIQ2SQYlNBZI5kCJj\ne3YfnW9ZgwZY76WW9IG1YvD13UZurrjXsv+mQIoDad1BjRcnNhVI5kByEJDsiwdprPRIcCqQ\ngKSQ6ZDaJ01PcCqQgKSQ4ZAmyyWJTgUSkBQyHNLVMjHRqUACkkJmQ5rXwPN7npQHJCApZDak\nO+XphKcCCUgKGQ1pZau9f0l4KpCApJDRkJ6Q/olPBRKQFDIZ0vrDUuYlPhVIQFLIZEgvyA0K\nU4EEJIVMhnRi8lcKU4EEJIUMhvSGXKUxFUhAUshgSGfLRxpTgQQkhcyFNEXOU5kKJCApZC6k\nyxN/dVBJQAKSQsZCmpF8fMKvDioJSEBSyFhIXeRFnalAApJCpkKam3LYep2pQAKSQqZCulXh\n5aqlAQlIChkK6aemrVcrTQUSkBQyFNJAGaw1FUhAUshMSCtb7bVUayqQgKSQmZAekbvUpgIJ\nSAoZCWnV/o0Xqk0FEpAUMhLSw/InvalAApJCJkJafUCTRXpTgQQkhUyE9JD0VZwKJCApZCAk\n3TskIAFJIwMhDZM7NacCCUgKmQdple4dEpCApJF5kP4q/VSnAglIChkH6ZfWundIQAKSRsZB\nGqr4ooaSgAQkhUyD9Mu+TX7UnQokIClkGqQhGu9SXCkgAUkhwyD9sm9T5TskIAFJI8MgPah+\nhwQkIGlkFqTFzVv8pD0VSEBSyCxIt8lf1acCCUgKGQVpUUqbVepTgQQkhYyC1EWe1Z8KJCAp\nZBKkGcntlN7LLjYgAUkhkyBdLP/yYSqQgKSQQZD+I+f6MBVIQNLIHEhZJ8ln+lOBBCSVzIE0\nVq6uelZzlYAEJIWMgbT28HrfAQlIQEqw4dKj6smYdQISkBQyBdLyfZvMBxKQgJRgd8rdVU/G\nrBSQgKSQIZC+TWmzAkhAAlKCXSovVD0Zs1ZAApJCZkD6l5yeBSQgASmx1hyZ/MkGIAEJSIk1\nRLoVfwISkICUQAubtyx5owYgAQlICXS9PFbyGUhAApL3Pko6Zm3JBpCABCTPZZ4qE0u3gAQk\nIHnu73LVri0gAQlIXlvaumHGrk0gAQlIXusu95ZtAglIQPLYR8m/WV22DSQgAclba45Jerf8\nApCABCRvDZSbKi4ACUhA8tTXDfdbUnEJSEACkpeyzpQXYy4CCUhA8tJTcknsRSABCUgeWtCy\nSUbsZSABCUgeSt31YtWygAQkILnvDTml8nvmAwlIQHLd0gMbfF55D5CABCTXdZYBVfYACUhA\nctvLcuKaKruABCQguWzRvinTqu4DEpCA5LLL5OFq+4AEJCC56yk5K7PaTiABCUiumtWs+ezq\ne4EEJCC5KfNseS7ObiABCUhuelAuj7cbSEACkov+l7LPwnj7gQQkIDnvl2Pk9bgHgAQkIDmv\ni3SPfwBIQLJtZ65dVqHtEg8V5PsxtajI+9c+Lydsin8kv8D71JordPCzd19exIehuRErz4ep\nOxP4v1VzBZbtTSvHB0g7tthlRWyXeCjX/ht7qLDI85d+3bjptzUcytnpeWotFVjZPkzdlu/D\n0C151jYfpmb7c8uycmzX+ACJh3alrThanq/pGA/teGgHJId1lt41HgMSkIDkrBFyctXXfFcE\nJCAByVFfNGoxq+ajQAISkJy0/MikCbUcBhKQgOSgrCvl9tqOAwlIQHLQPXLm2tqOAwlIQLJv\nQtIhcV9iVx6QgAQk2z5t3PTz2lcACUhAsmv+gUkv2iwBEpCAZNPq9vKg3RogAQlItZd1jaRl\n2S0CEpCAVHv3y29X2i4CEpCAVGsTktv8YL8KSEACUm1Natj4vw6WAQlIQKqlaS3rvexkHZCA\nBKSam7V/0t8dLQQSkIBUYz8eKX91thJIQAJSTS3/rfR1uBRIQAJSDa25SDrb/gPSroAEJCDF\nLzNNLlvndCqQgASkuGWmy+n2/xBbFpCABKR4ZXaVE35yPhVIQAJSnDK7yAmLXUwFEpCAVL31\nLh0BCUhAqt766+VEV46ABCQgVcu9IyABCUhVW3+tnLLU5VQgAQlIlVt1hZzq1hGQgASkyi05\nU85a5noqkIAEpNjmHy+Xr3I/FUhAAlJM0w+Wro5fFxQTkIAEpIo+biX9PE0FEpCAVN7rjes9\n6W0qkIAEpLIertfoVY9TgQQkIJW2+nrZ50OvU4EEJCCV9EN7OS7D81QgAQlIxX16kKT94n0q\nkIAEpGgvNErql5nAVCABCUgb1t4qe72e0FQgAQlIGe3liOmJTQUSkEIP6cXmkur6VapVAhKQ\nQg5p9S3S8NGEpwIJSOGGNL2dHP1F4lOBBKQwQ8oa0Ui6eXixd7WABKQQQ5p9obQYrzIVSEAK\nLaSsJ5rJBbN1pgIJSGGFlHGe7PW00/f2tgtIQAonpKynm8rFc9SmAglIoYT02enS8lnFqUAC\nUgghLe5RT66YrzkVSEAKHaTM5/aRtv/UnQokIIUN0rftpfHA1cpTgQSkcEHKSE+Sq+eqjwUS\nkMIEaWHvFDnhPfWxQAJSmCCtGLyXHPR8vvLUkoAEpLBAWj2slezzyBoXZzV3EZCAFA5Iy4fu\nJ80GLndzVnM3AQlIYYC0+N6W0qTPj8WbQAISkLy1cGBzaXbLgtILQAISkLz0adcUaT3457KL\nQAISkFy3dtwZIoc+HvO3e0ACEpBctmjwQSJnjK90nhYgAQlIblr/RmqKNL256vtsAQlIQHLe\nN/3biPzm0eonsQQSkIDksGX/ODNJmnb9IN7fvwIJSEBy0ooxl6dI0lnProh/GEhAApJtq15K\nbRR9SHfPzBpXAAlIQKq9H5+9qonIoXd9VtsiIAEJSLU0fcgZ9UQO6TPFZh2QgASkGlr2ys2H\niySf+oCDNx8GEpCAFKd1Hw48vb5I49+PdPZuJkACEpCqtOaDBy9uFr0rOumud9c4nQokIAEp\npiX/vufshiLS9oZxi91MBRKQgFTa2k+GX/ebJJGkY3uOmed2KpCABKQNqz95unv74juiJmf/\n6RVX90RlAQlI4YY0Z+Jfr2tXP2qo3rFdn/p8XQ1fZBuQgBRSSFmz33qs2xktooQk5eSbRny0\nMqGpQAJS6CAtmDxqQNrxjYoJJbe9vP+YaWsTnwokIIUG0vJp/3y8zxXHN5GSe6F2V/V/bmpi\nd0MxAQlIQYe06pv3Rw++5Q/HtywBJI2OueyWxyZmZKrMLg9IQAokpMyFX747+qE+nc8/dpcf\nSTni/K5/fvb9udt2x1nNlQISkOoA0pLvprw5+tF7bk4755h9k3bxkca/Obdzv+Evfzy/7M/x\ndsNZzdUCUtghRcb36j4qv/J27D4NSL8s/G7qe68+N/z+P3VLu+Ckw8rueYprcuhpv+/a/7EX\n/+/r6n+HByQgmQNpbI+ZGb1HVt6O3ecK0rKfZn0zdfLbr4x7+rHB/ft06/DHC046+pCWSRJb\ng32PbH/xNb3/PHzMxE+/X1XbVCAByRhIOzp/ZVmzOm6J3Y7d5wxS1smHHdqypcQpqeWBR518\nwZXX3dxv8JNj3vhg2pwa/gA87n8ukIBkCqRFqdstqyBtdux2zL6i7GibN9q1oWnLNocde9KZ\nF1xx9U09+t0z5PFnxv/znU+mZSxZbfultZSzPZGvrqlIkR9Tt+X6MTXf+tWHqVvyfBi6Mdfa\n4sPUXwt8GLoxx9pqu8YlpBkdiz+mT43djtm3qX20MY4mEQWoSPmWM0jTOxV/TJ8Sux2zb1uf\naJPy7bKKbJd4KBLxY2qR5cfUSKEfU/25rgW+XNdCq8CPsf7csizbm9ZOl5AWpe6I6kvLiN2O\n3VdcXb/WTjeeI/EcyYfnSDnXzrSseR02xW7H7gOSs4AUckjWC7cvXdbvGcuaOrliu+wzkBwH\npLBDiozt2X10vmUNGlCxXfYZSI4DUtghOQhI9gEJSEBSCEhAApJCQAISkBQCEpCApBCQgAQk\nhYAEJCApBCQgAUkhIAEJSAoBCUhAUghIQAKSQkACEpAUAhKQgKQQkIAEJIWABCQgKQQkIAFJ\nISABCUgKAQlIQFIISEACkkJAAhKQFAISkICkEJCABCSFgAQkICkEJCABSSEgAQlICgEJSEBS\nCEhAApJCQAISkBQCEpAUIL34tsZ/XtW2+PF/ZsMbL/sxdXO2H1P/b9w6H6b+an8r8tBn45b6\nMHXjdh+Gbpg5bp7tGh8g2XfmjbvxmyVYp4vq+ho478722XV9FRz3RPuFdX0VHPda+/86Xwyk\n+AHJn4CkEJD8CUj+BCSFgORPQFIoe/tu/GYJtn1rXV8D5+VkF9X1VXBcbnZhXV8Fx+Vl59sv\nKmt3QiIKbEAiUghIRAoBiUih3Qpp6oDrBq3end8wgVYN63rjExvs19V5kfG9uo9y8ay4LjPm\nh1qcu1vr7oQ0tfN/5w66zYzf2uTf8viSmQPvqeur4aCxPWZm9B5Z19fCUeb8UC3Xt9bdCKno\n9g8sa8PjmbvvOybQ4tRtljU3Nbeur4dtOzp/ZVmzOm6p6+vhJGN+qJb7W+tuhLQy9dciI/5/\nF1eYa+X+PPruur4a9i1K3W5ZBWmz6/p6OMmYH6rl/ta6GyF93+Gd61K7T9993zDB/pLadWVd\nXwf7ZnQs/pg+ta6vh8PM+KFa7m+tuwfS9NTU1NVfpD6amfNWxz3+51hyZaOft2a+dsOOur4y\ntk3vVPwxfUpdXw+HmfFDjeb21rp7IEVycnKK5qRuim72en+3fMcEKrmyKzKiW0XXzqzrK2Pb\notTo7TKSllHX18NJxvxQo7m9te7Gh3Yb0qK8Izea8SDkfzdGLGu7AbfPnOLb5bwOm+r6ejjJ\nmB+q5f7Wujt//f1E/zlLnupuxqtBs9OfWbJwyG076/p62PfC7UuX9Xumrq+Fo8z5oVqub627\nE1LeqJ7pD6/Zjd8wkRbfd323ESb8qj4ytmf30Yb8g6wxP1TL9a2VlwgRKQQkIoWARKQQkIgU\nAhKRQkAiUghIRAoBiUghIBEpBCQihYBEpBCQiBQCEpFCQDK87+oVvyvPo8lf1vUVCXlAMr0/\n18uwfmrUv66vRtgDkunlHn1qwYVH5tT11Qh7QDK+aUkX8MCuzgOS+d0hf6rrq0BAMr8r5Vxz\nzjQW1IBkfC9LPxld11ci9AHJ9Na0vNHq0NyUk3wENiCZ3pX7brBWNUur66sR9oBkeK/Iq9GP\nf5OJdX1FQh6QiBQCEpFCQCJSCEhECgGJSCEgESkEJCKFgESkEJCIFPp/lFVznPOn/jYAAAAA\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ggplot2)\n",
    "library(tensorflow)\n",
    "tfe_enable_eager_execution(device_policy = \"silent\")\n",
    "\n",
    "ggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \n",
    " stat_function(fun = function(x) tf$sigmoid(x)$numpy()) +\n",
    " xlim(-6,6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting large values into the sigmoid throws us into a region where the function is almost flat, so that the gradients become essentially 0. In particular, any information collected by the backpropagation algorithm is lost after this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One embarassingly simple solution to this problem is to replace the sigmoid with the activation function $\\max(x, 0)$, also known as **Rectified Linear Unit (ReLU)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nDEz9eeviGQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(data = data.frame(x = 0), mapping = aes(x = x)) + \n",
    " stat_function(fun = function(x) tf$nn$relu(x)$numpy()) +\n",
    " xlim(-1,3) +\n",
    " ylim(0,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD ++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have encountered stochastic-gradient descent (SGD) as the standard method to fit neural networks to data. Despite its simplicity, SGD is still the foundation of many state-of-the-art optimizers. In this notebook, we learn how to improve speed and accuracy of SGD with simple tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several iterations of SGD the weights gradually approach the optimum. However, at a certain point the gradient steps become too coarse to achieve further improvements. Once this happens, the learning rate should be reduced to arrive at a better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Adagrad**, developed by [John Duchi, Elad Hazan, Yoram Singer](http://jmlr.org/papers/v12/duchi11a.html), achieves this goal by reducing the learning rate with successive gradient steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, writing $g^j = \\nabla \\ell(w^j)$ for the gradient of the loss function in the $j$th iteration, we let \n",
    "$$G^i = {\\sum_{j \\le i} (g^j)^2}$$\n",
    "denote the sum of squared gradients in the first $i$ steps. Then, the Adagrad update step becomes\n",
    "$$w^{i+1} = w^{i} - \\frac\\alpha{\\sqrt{G^{i}}}g^{i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual implementation, Adagrad introduces a separate learning rate for each weight, so as to take into account situations where some converge faster than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, Adagrad often results in a highly aggressive reduction of the learning rate and therefore into slow learning. **RMSProp** developed by [Geoff Hinton](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) resolves this problem by replacing the sum of squared gradients with a moving average:\n",
    "$$G^{i+1} = .9 \\, G^{i} + .1\\, ( g^{i})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we put\n",
    "$$w^{i+1} = w^{i} - \\frac\\alpha{\\sqrt{G^{i}}}g^{i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentum** is a [classical technique](https://www.sciencedirect.com/science/article/pii/0041555364901375?via%3Dihub) in optimization to dampen oscillations appearing in the course of gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/momentum.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to smoothen the gradients by replacing gradients via moving averages. Using a parameter $\\beta<1$, we introduce the momentum vector\n",
    "$$m^{i+1} = \\beta m^i + g^i.$$\n",
    "and define the update step via\n",
    "$$w^{i+1} = w^i - \\alpha m^i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabriel Goh's [Distill article](https://distill.pub/2017/momentum/) provides beautiful interactive illustrations and a mathematical proof for momentum on the vanilla SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam** was developed by [Diederik P. Kingma and Jimmy Ba](https://arxiv.org/abs/1412.6980) and augments RMSProp with momentum. More precisely, we put\n",
    "$$z^{i+1} = \\beta_1 m^i + (1 - \\beta_1) g^i,$$\n",
    "$$G^{i+1} = \\beta_2 G^i + (1 - \\beta_2) (g^i)^2,$$\n",
    "\n",
    "and then define the update step\n",
    "$$w^{i+1} = w^i - \\frac\\alpha{\\sqrt{G^i}}m^i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implementation also contains bias-corrections for $m^i$ and $G^i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical optimization gradient descent approaches are typically superseded by methods incorporating information on the Hessian, such as the Newton-Raphson algorithm. This has the advantage of speeding up convergence substantially. There are three problems, why this reasoning does not carry over to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Even moderately-sized networks consist of millions of weights, so that the number of entries in the Hessian goes into the trillions. This means the death, unless highly sophisticated heuristics are used to reduce the number of weights. \n",
    "2. The Hessian is typically [ill-conditioned in deep learning](https://arxiv.org/abs/1706.04454). That is, the vast majority of eigenvalues are close to 0 and matrix inversion is numerically volatile. \n",
    "3. The number of training examples that are available nowadays is typically very large, in particular if techniques like data augmentation are used. Spending more computational time on refined optimization methods means that the algorithm can see less training data. Experience shows that the time gained by faster convergence is typically not worth the price paid by seeing less examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras makes it very convenient to specify the optimization algorithm to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "\n",
    "input_dim <- 5\n",
    "hidden_dim <- 32\n",
    "\n",
    "lm <- keras_model_sequential() %>% \n",
    "  layer_dense(units=32, input_shape = input_dim) %>%\n",
    "  layer_dense(units=1, activation='sigmoid')\n",
    "\n",
    "lm %>%\n",
    "    compile(optimizer=optimizer_adam(lr=1e-3), \n",
    "            loss='binary_crossentropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
