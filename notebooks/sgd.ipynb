{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in other regression models, we have to deal with the question of how to fit the parameters of the MLP. In the language of neural networks, we speak of **training the model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard approach to fitting parameters is *maximum-likelihood estimation (MLE)*. For instance, consider a *linear model* $y = wx + \\mathcal{N}(0, 1)$ with weight vector $ w = (w_1,\\ldots, w_p)^\\top$ and i.i.d. training data $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$. Writing $y = (y_1, \\ldots, y_n)^\\top$ and $X = (x_1, \\ldots, x_N)^\\top$ the log-likelihood $l$ is, given by the quadratic function\n",
    "$$l(w) =  \\sum_{i \\le N}\\log p_{\\mathcal{N}(0,1)}(y_i - w^\\top x_i) = -\\frac12 \\sum_{i} (y_i - w ^\\top x_i)^2  + C = -\\frac12 \\big(y^\\top y - 2 y^\\top Xw + (Xw)^\\top Xw\\big) + C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of deep learning, the negative log-likelihood $L(w)=-l(w)$ is called **loss function**. \n",
    "with gradient\n",
    "$$\\nabla_w L = - X^\\top y + X^\\top Xw.$$\n",
    "Solving $\\nabla_w l =0$, minimization of the loss function results in $w = (X^\\top X)^{-1} X^\\top y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the total loss function accumulates additively losses over all samples in the training data:\n",
    "$$L(w) = \\sum_{i \\le N} \\ell(y_i, f_w(x_i)).$$\n",
    "The linear model uses the *mean-squared error*: $\\ell(y, y') = \\tfrac12 (y - y')^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the model is non-linear, then the log-likelihood is typically no longer quadratic in the weights $\\theta$ and a closed-form minimization of $L(\\theta)$ is typically no longer possible. We therefore have to resort to numerical optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most na√Øve optimization procedure for minimizing the loss function $L$ is **gradient descent**, where we iteratively change the weights by taking steps in the direction of the negative gradient $-\\nabla_w L$ of the loss function:\n",
    "$$w^{(i+1)} = w^{(i)} - \\alpha \\nabla_w L(w^{(i)}).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradientDesc.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Google Machine Learning Course](./images/learnRate.html) provides a nice illustration of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step size $\\alpha > 0$ is called **learning rate**. In contrast to Newton-Raphson, the learning rate is not computed as a function of the current weights $w^{(i)}$, but rather tuned manually. Whereas choosing the learning rate too large can cause instabilities and failure of convergence, choosing the learning rate too small leads to long convergence times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Gradient Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MLP $f_w$ consists of many hidden layers, then computing the gradient by hand is painful. A key asset of tensorflow is the automated gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-55d22336413e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5466\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5468\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works for the linear model. For simplicity, we only keep the matrix weights and leave the bias aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "\n",
    "class LM(tfe.Network):  \n",
    "    def __init__(self):    \n",
    "        super(LM, self).__init__()   \n",
    "        self.W = tfe.Variable(tf.random_uniform((1,input_dim), -1, 1))\n",
    "    \n",
    "    def call(self, input):   \n",
    "            result = tf.matmul(self.W, input)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can now compute the value of the model given the initial parameters. To make it more comparable to a later setting, we increase the sample size to 1,000,000. We also make an assumption of a true model and generate corresponding response variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "input_size = int(1e6)\n",
    "\n",
    "model = LM()\n",
    "x = tf.random_uniform((input_dim, input_size), -1, 1)\n",
    "y_pred = model(x)\n",
    "\n",
    "w_true = [[1., 2., 3., 4., 5.]]\n",
    "y = tf.matmul(w_true, x) + tf.random_uniform((1, input_size), -.1, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loss function, we use mean-squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, x, y):  \n",
    "    y_ = model(x)\n",
    "    return tf.losses.mean_squared_error(y, y_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, the weight matrix is far away from its true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=\n",
       "array([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, we can retrieve both the gradient and the actual values of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: id=113, shape=(1, 5), dtype=float32, numpy=\n",
       "  array([[-0.44741115, -1.4015851 , -2.1849227 , -2.7139482 , -3.949926  ]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=\n",
       "  array([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],\n",
       "        dtype=float32)>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfe.implicit_gradients(loss_function)(model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After taking 500 gradient steps the weights are much closer to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=\n",
      "array([[0.9762546, 1.9234493, 2.881829 , 3.8563163, 4.7897825]],\n",
      "      dtype=float32)>\n",
      "CPU times: user 36.9 s, sys: 9.91 s, total: 46.8 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(500):\n",
    "    grad = tfe.implicit_gradients(loss_function)(model, x, y)[0][0]\n",
    "    model.W.assign_sub(.01 * grad)\n",
    "print(model.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using backpropagation as optimization method has important consequences on also on the choice of initial weights. Indeed, if all weights and biases are initialized as 0, then gradients tend to vanish or degenerate and no learning is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hence, we need to rely on a random initialization. However, if the scale of the random initial weights is chosen too large, then gradient descent results into exploding gradients. What has been found to work well in practice is the [Xavier-Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), where weights are initialized in the interval $[-\\sqrt{6}/\\sqrt{d_\\mathsf{in}+d_\\mathsf{out}},\\sqrt{6}/\\sqrt{d_\\mathsf{in}+d_\\mathsf{out}}]$, where $d_\\mathsf{in}$ and $d_\\mathsf{out}$ denote the input and output dimensions of the considered layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\ms{\\mathsf}$\n",
    "Loosely speaking, in the forward pass the variance of the randomness at a neuron in the output is $d_\\mathsf{out}\\mathsf{Var}(w_{ij})$. Similarly, in the backward pass, we would obtain a restriction $d_\\mathsf{in}\\mathsf{Var}(w_{ij})$ should be constant. A compromise between the two conditions leads to the asserted initialization proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Iris classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a single-layer MLP for the classification problem in the iris dataset and optimize it via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize over the *cross-entropy loss*. If $(p_1, p_2, p_3)$ are the predicted class probabilities and $y$ is the true outcome, then this loss is given by \n",
    "$$\\ell(y, \\{p_i\\}_{i \\le 3}) = -\\sum_{i=1}^3 \\mathbb{1}\\{y = i\\} \\log p_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the values of the final layer correspond to the class probabilities, optimizing cross-entropy is equivalent to maximum-likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that if there is a lot of training data, then computing gradients iteratively can be prohibitively time-consuming. The idea of **stochastic gradient descent (SGD)** is to select only a subset $J$  of all training indices. The associated sub-sample is called **mini-batch**. Instead of computing the loss based on all training data, only the data in the mini-batch is used. That is, \n",
    "$$L_{\\mathsf{SGD}}(w) = \\sum_{i \\in J} \\ell(y_i, f_w(x_i)).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works out for the linear model considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 5) dtype=float32, numpy=\n",
      "array([[0.9715018, 1.9286776, 2.8780506, 3.8595467, 4.7856936]],\n",
      "      dtype=float32)>\n",
      "CPU times: user 746 ms, sys: 0 ns, total: 746 ms\n",
      "Wall time: 748 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy.random as random\n",
    "batch_size = 128\n",
    "tf.set_random_seed(42)\n",
    "model = LM()\n",
    "\n",
    "for i in range(500):\n",
    "    xb = tf.slice(x, [0, i * batch_size], [input_dim, batch_size])\n",
    "    yb = tf.slice(y, [0, i * batch_size], [1, batch_size])\n",
    "    \n",
    "    grad = tfe.implicit_gradients(loss_function)(model, xb, yb)[0][0]\n",
    "    model.W.assign_sub(.01 * grad)\n",
    "print(model.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we arrive almost at the same accuracy in a fraction of the time. Moreover, the randomness injected by choosing only to optimize over mini-batches helps to fight overfitting and leads to solutions that generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `tensorflow` does a great job computing gradients for us. But what actually happens, when calling `implicit_gradients`? [Recall](./mlp.ipynb) that an MLP can be written in the form\n",
    "$$f(x) = a_L(W_L \\cdot a_{L-1}(W_{L-1}\\cdots (W_2 \\cdot a_1(W_1 \\cdot x + b_1) + b_2)\\cdots) + b_{L})$$\n",
    "with weights $W_i \\in \\mathbb{R}^{\\ell_i \\times \\ell_{i-1}}$, biases $b_i \\in \\mathbb{R}^{\\ell_i}$ and activation functions $a_i:\\mathbb{R}^{\\ell_i} \\to \\mathbb{R}^{\\ell_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write $f$ more succinctly as a composition of functions \n",
    "$$f(x) = g^{(L)}_{W_L, b_L}(\\cdots (g^{(2)}_{W_2, b_2}(g^{(1)}_{W_1, b_1}(x)))),$$\n",
    "where $g^{(i)}_{W_i, b_i}(z) = a_i(W_i z + b_i)$. Since $f$ is a composition of functions, the gradients can now be computes by invoking the chain rule. This technique of automatic differentiation is called **backpropagation** and goes back to a [seminal paper](https://www.nature.com/articles/323533a0) by David E. Rumelhart, Geoffrey E. Hinton & Ronald J. Williams published in 1986."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Google Machine Learning Crashcourse](./images/backProp.html) provides a beautiful illustration for backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backProp.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain the general idea in the case where $L = 2$ without biases: $b_2 = b_1 = 0$. Since $a_2$ is typically a loss function, we assume the last layer to be one-dimensional, i.e., $\\ell_2=1$. To simplify notation, we write $\\ell = \\ell_1$ and $m = \\ell_0$. Hence, $W_2 = (w_{2;1}, w_{2;2}, \\ldots, w_{2;\\ell})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, writing $\\big(g^{(1)}_{W_1}(x)\\big)_k$ for the $k$-th component of $g^{(1)}_{W_1}(x)$, the chain rule gives\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_{2;j}}a_2(W_2 g^{(1)}_{W_1}(x))&= \\frac{\\partial}{\\partial w_{2;j}}a_2\\Big(\\sum_k w_{2;k} \n",
    "\\big(g^{(1)}_{W_1}(x)\\big)_k\\Big)\\\\\n",
    "&= a_2'\\Big(\\sum_k w_{2;k} \n",
    "\\big(g^{(1)}_{W_1}(x)\\big)_k\\Big) \\frac{\\partial}{\\partial w_{2;j}}\\sum_k w_{2;k} \n",
    "\\big(g^{(1)}_{W_1}(x)\\big)_k\\\\\n",
    "&= a_2'\\Big(W_2 g^{(1)}_{W_1}(x)\\Big)\\big(g^{(1)}_{W_1}(x)\\big)_j\n",
    "\\end{align*}$$\n",
    "Hence, \n",
    "$$\\nabla_{W_2} f(x) =  a_2'(W_2 g^{(1)}_{W_1}(x))g^{(1)}_{W_1}(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expressions $W_2 g^{(1)}_{W_1}(x)$ and $g^{(1)}_{W_1}(x)$ were already evaluated when computing $f(x) = a_2(W_2 a_1(W_1x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the partial derivatives is similar with the exception that we now also have to apply the chain rule for $g_{W_1}^{(1)}$:\n",
    "$$\\frac{\\partial}{\\partial w_{1;i,j}}a_2(W_2 g^{(1)}_{W_1}(x)) = a_2'\\Big(W_2 g^{(1)}_{W_1}(x)\\Big) \\frac{\\partial}{\\partial w_{1;i,j}}\\Big(W_2 g^{(1)}_{W_1}(x)\\Big)= a_2'\\Big(W_2 g^{(1)}_{W_1}(x)\\Big) W_2 \\frac{\\partial}{\\partial w_{1;i,j}}g^{(1)}_{W_1}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we again have already computed $a_2'\\Big(W_2 g^{(1)}_{W_1}(x)\\Big)$ when computing gradients with respect to $W_2$. Hence, we can proceed recursively and it only remains to compute the gradients for $g^{(1)}_{W_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculations already provide a very strong hint that when computing $f(x)$ and its gradients $f(x)$ many intermediate should be stored and re-used at a later stage. More precisely, the training via SGD decomposes into a **forward pass** and a **backward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute $f(x)= g^{(L)}_{W_L}(\\cdots (g^{(2)}_{W_2}(g^{(1)}_{W_1}(x))))$ recursively:\n",
    "\n",
    "* compute $g^{(1)}_{W_1}(x)$ and save the value as $h^{(1)}$\n",
    "* compute $g^{(2)}_{W_2}(h^{(1)})$ and save the value as $h^{(2)}$\n",
    "* continue recursively until arriving at $f(x) = g^{(L)}_{W_L}(h^{(L-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the **forward phase** since we start with the computation of the innermost expression and make our way forward until we arrive at the outermost position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward phase, we compute the gradients of $\\nabla_W f(x)$ via the chain rule. Here, we start by computing the outermost expressions and make our way to the innermost expression. \n",
    "* To compute the gradients $\\nabla_{W_L} f(x)$ with respect to the outermost weights, we proceed as above to see that $\\nabla_{W_L} f(x) = (\\nabla_{W_L} g^{(L)}_{W_L})(h^{(L-1)}) \\cdot h^{(L-1)}$. \n",
    "* To compute the gradients $\\nabla_{W_{L-1}} f(x)$ with respect to $W_{L-1}$, we proceed as above to see that $\\nabla_{W_{L-1}} f(x) = (\\nabla g^{(L)}_{W_L})(h^{(L-1)}) \\cdot (\\nabla_{W_{L-1}} g^{(L-1)}_{W_{L-1}})(h^{(L-2)})$. We save the expression $(\\nabla g^{(L)}_{W_L})(h^{(L-1)})$\n",
    "* To compute the gradients $\\nabla_{W_{L-2}} f(x)$ with respect to $W_{L-2}$, we proceed as above to see that $\\nabla_{W_{L-2}} f(x) = (\\nabla g^{(L)}_{W_L})(h^{(L-2)}) \\cdot (\\nabla g^{(L-1)}_{W_{L-1}}(h^{(L-2)}) (\\nabla_{W_{L-1}} g^{(L-2)}_{W_{L-2}})(h^{(L-3)})$. We can reuse the expression $(\\nabla g^{(L)}_{W_L})(h^{(L-1)})$ from the previous step and  save additionally the expression $\\nabla g^{(L-1)}_{W_{L-1}}(h^{(L-2)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already see that the computations can get rather involved. All the details are presented in  [Chapter 6.5 of the monograph on Deep Learning](http://www.deeplearningbook.org/) by  Aaron Courville, Ian J. Goodfellow, and Yoshua Bengio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upshot of the backpropagation algorithm is that one training step in the backpropagation algorithm takes essentially one forward pass and one backward pass through the network. Hence, it takes roughly twice as long as a simple evaluation. However, as we will see in the practical examples, many passes over the data might be needed until the weights have converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing & Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have walked through SGD, we can understand better why it was not possible to build deep nets in the 1980s and 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule causes many multiplications as a gradient travels through the multiple layers. If the multiplications are not precisely fine-tuned, then the gradients become numerically unstable. To understand what happens when plugging in numerically unstable values into a sigmoid function, let's plot it once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np  \n",
    "%matplotlib inline\n",
    "\n",
    "t = np.arange(-5, 5, 0.01)  \n",
    "s = 1/(1 + np.exp(-t))\n",
    "plt.plot(t, s)  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting large values into the sigmoid throws into a region where the function is almost flat, so that the gradients become essentially 0. In particular, any information collected by the backpropagation information will be lost after this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One embarassingly simple solution to this problem is to replace the sigmoid with the activation function $\\max(0, x)$, also known as **Rectified Linear Unit (ReLU)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np  \n",
    "%matplotlib inline\n",
    "\n",
    "t = np.arange(-5, 5, 0.01)  \n",
    "s = np.maximum(t, np.zeros(len(t)))\n",
    "plt.plot(t, s)  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this function is that if we insert large positive values, the gradient never vanishes but remains constant at 1. This simple trick helps a lot for being able to send gradients through many layers without them getting killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Backpropagation - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish the computation of $\\nabla_{W_1}f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the gradient computation involve backpropagation? Couldn't we just as well compute in a feed-forward manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it not be better to choose a linear activation function $a(x) = c \\cdot x$ instead of ReLU? Here, there is never a problem of a vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Backpropagation - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backpropagation with $L = 2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
