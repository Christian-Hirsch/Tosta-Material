{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final notebook of this course, we explore a particularly amusing application of GANs: **Pix2Pix**, originally going back a paper by [Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"gab\" src=\"images/e2c.jpg\"  width=\"700\">\n",
    "https://knowyourmeme.com/photos/1225207-edges2cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Pix is a generative model in the sense that it employs deep learning to generate images. However, the generated images are not entirely arbitrary, but are rather trained to satisfy a consistency property. For instance, the above example is trained on a large number of cat images together with a corrspondingly sketched outline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"gab\" src=\"images/e2c.png\"  width=\"700\">\n",
    "https://laughingsquid.com/edges2cats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the generative model is trained, it creates what it believes is the closest thing coming to the picture of a cat under the constraint of the sketched outline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the principle we follow the [outstanding blogpost](https://affinelayer.com/pix2pix/ ) by Christopher Hesse. We also discuss code snippets from the associated [GitHub repository](https://github.com/affinelayer/pix2pix-tensorflow). For the purpose of presentation, we often provide just pseudo-code and leave out technical details. Everybody is encouraged to see all the details in the [pix2pix.py-file](https://affinelayer.com/pix2pix/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following illustrations from [Christopher Hesse's blogpost](https://affinelayer.com/pix2pix/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img id=\"gab\" src=\"images/p2pInp.png\"  width=\"200\"></td>\n",
    "        <td><img id=\"gab\" src=\"images/p2pOut.png\"  width=\"200\"></td>\n",
    "    </tr>\n",
    "    </table>\n",
    "https://laughingsquid.com/edges2cats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the task is to train a network for the purpose of image colorization. As training data, we give the color images -- called *targets* -- and their gray-scale transformations -- called *inputs*. The purpose of the generator is to transform an unseen input into a correctly colored *output*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing the precise architecture of discriminator and generator, we quickly go over the training mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the training follows the classical GAN paradigm. The discriminator is trained via cross-entropy, whereas for the generator we rely on the standard adaption to avoid the vanishing-gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do stress however, that in addition to the adversarial loss, the generator also contains an L1-reconstruction loss thereby enforcing that the generated output is close to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, targets):\n",
    "    #create generator\n",
    "    outputs = create_generator(inputs, targets.get_shape()[-1])\n",
    "    \n",
    "    #create discriminators\n",
    "    predict_real = create_discriminator(inputs, targets)\n",
    "    predict_fake = create_discriminator(inputs, outputs)\n",
    "    \n",
    "    #discriminator loss\n",
    "    discrim_loss = tf.reduce_mean(-(tf.log(predict_real) + tf.log(1 - predict_fake)))\n",
    "    \n",
    "    #generator loss\n",
    "    gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake))\n",
    "    gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "    gen_loss = gen_loss_GAN + gen_loss_L1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the discriminator with an input and a target. The task of the discriminator is to find out whether target and the input come from a real data pair or whether the target was produced by the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this goal, the discriminator is designed as deep CNN. As discussed in the notebook on GANs, we see peculiarities in comparison to CNNs used in classification. For instance, stride-2 convolution replaces the use of max-pooling and Leaky ReLUs are used instead of ordineary ReLUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(discrim_inputs, discrim_targets):\n",
    "        input = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "                convolved = discrim_conv(layers[-1], out_channels, stride=2)\n",
    "                normalized = batchnorm(convolved)\n",
    "                rectified = lrelu(normalized, 0.2)\n",
    "                layers.append(rectified)\n",
    "                \n",
    "        return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the task of the generator to convert an uncolored image into a colored image. Hence, both input and output, so that it makes sense to rely on a [U-net architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png) typically used in image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"gab\" src=\"images/unet.png\"  width=\"700\">\n",
    "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half of the U-net is also known as *encoder*, whereas the second one is typically called *decoder*. Between the decoder and the corresponding encoder layer skip-connections are used to transfer information directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(generator_inputs, generator_outputs_channels):\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "            convolved = gen_conv(rectified, out_channels)\n",
    "            output = batchnorm(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "\n",
    "        input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "        rectified = tf.nn.relu(input)\n",
    "        output = gen_deconv(rectified, out_channels)\n",
    "        output = batchnorm(output)\n",
    "\n",
    "        layers.append(output)\n",
    "        \n",
    "    return layers[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
